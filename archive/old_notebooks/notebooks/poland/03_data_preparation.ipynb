{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "## Objective\n",
    "\n",
    "Prepare data for modeling:\n",
    "1. **Handle remaining issues** - Missing values, outliers\n",
    "2. **Feature selection** - Remove highly correlated features for linear models\n",
    "3. **Scaling** - Standardize features\n",
    "4. **Train/test splits** - Proper validation strategy\n",
    "\n",
    "## Note\n",
    "\n",
    "Your data appears already preprocessed (from existing notebooks).\n",
    "This notebook documents the preparation strategy and creates clean splits for modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.bankruptcy_prediction.data import DataLoader, MetadataParser\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = DataLoader()\n",
    "metadata = MetadataParser.from_default()\n",
    "\n",
    "# Load both full and reduced datasets\n",
    "df_full = loader.load_poland(horizon=1, dataset_type='full')\n",
    "df_reduced = loader.load_poland(horizon=1, dataset_type='reduced')\n",
    "\n",
    "print(f\"Full dataset: {df_full.shape}\")\n",
    "print(f\"Reduced dataset: {df_reduced.shape}\")\n",
    "print(f\"\\nFull dataset is for Random Forest (handles multicollinearity)\")\n",
    "print(f\"Reduced dataset is for Logistic/GLM (correlation pruned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Quality Check\n",
    "\n",
    "Verify preprocessing quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, y = loader.get_features_target(df_full)\n",
    "X_reduced, _ = loader.get_features_target(df_reduced)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Full Dataset:\")\n",
    "print(f\"  Features: {len(X_full.columns)}\")\n",
    "print(f\"  Missing values: {X_full.isnull().sum().sum():,}\")\n",
    "print(f\"  Infinite values: {np.isinf(X_full.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "print(f\"\\nüìä Reduced Dataset:\")\n",
    "print(f\"  Features: {len(X_reduced.columns)}\")\n",
    "print(f\"  Missing values: {X_reduced.isnull().sum().sum():,}\")\n",
    "print(f\"  Infinite values: {np.isinf(X_reduced.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "print(f\"\\nüìä Target:\")\n",
    "print(f\"  Samples: {len(y):,}\")\n",
    "print(f\"  Bankruptcy rate: {y.mean():.2%}\")\n",
    "print(f\"  Class balance: {y.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "\n",
    "**Data appears preprocessed with:**\n",
    "- ‚úÖ Missing values handled (imputation or indicators)\n",
    "- ‚úÖ No infinite values\n",
    "- ‚úÖ Two feature sets prepared (full vs reduced)\n",
    "\n",
    "**Reduced dataset:**\n",
    "- Correlation pruning applied (removed features with r > 0.9)\n",
    "- Suitable for Logistic Regression and GLM\n",
    "- Prevents multicollinearity issues\n",
    "\n",
    "**Full dataset:**\n",
    "- All features retained\n",
    "- Suitable for Random Forest (not affected by correlation)\n",
    "- Maximum information preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split Strategy\n",
    "\n",
    "Create stratified splits for reproducible evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split parameters\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Split strategy: {(1-TEST_SIZE)*100:.0f}% train, {TEST_SIZE*100:.0f}% test\")\n",
    "print(f\"Random seed: {RANDOM_STATE}\")\n",
    "print(f\"Stratification: YES (preserve class balance)\\n\")\n",
    "\n",
    "# Split full dataset (for Random Forest)\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "    X_full, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split reduced dataset (for Logistic/GLM)\n",
    "X_train_reduced, X_test_reduced, _, _ = train_test_split(\n",
    "    X_reduced, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"‚úì Splits created:\\n\")\n",
    "print(f\"Full dataset:\")\n",
    "print(f\"  Train: {X_train_full.shape} ({len(X_train_full):,} samples)\")\n",
    "print(f\"  Test:  {X_test_full.shape} ({len(X_test_full):,} samples)\")\n",
    "\n",
    "print(f\"\\nReduced dataset:\")\n",
    "print(f\"  Train: {X_train_reduced.shape} ({len(X_train_reduced):,} samples)\")\n",
    "print(f\"  Test:  {X_test_reduced.shape} ({len(X_test_reduced):,} samples)\")\n",
    "\n",
    "print(f\"\\nClass balance preserved:\")\n",
    "print(f\"  Train set: {y_train.mean():.2%} bankrupt\")\n",
    "print(f\"  Test set:  {y_test.mean():.2%} bankrupt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling\n",
    "\n",
    "Standardize features for models that require it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on training data only (prevent data leakage)\n",
    "scaler_full = StandardScaler()\n",
    "scaler_reduced = StandardScaler()\n",
    "\n",
    "X_train_full_scaled = scaler_full.fit_transform(X_train_full)\n",
    "X_test_full_scaled = scaler_full.transform(X_test_full)\n",
    "\n",
    "X_train_reduced_scaled = scaler_reduced.fit_transform(X_train_reduced)\n",
    "X_test_reduced_scaled = scaler_reduced.transform(X_test_reduced)\n",
    "\n",
    "# Convert back to DataFrames with column names\n",
    "X_train_full_scaled = pd.DataFrame(X_train_full_scaled, \n",
    "                                    columns=X_train_full.columns,\n",
    "                                    index=X_train_full.index)\n",
    "X_test_full_scaled = pd.DataFrame(X_test_full_scaled, \n",
    "                                   columns=X_test_full.columns,\n",
    "                                   index=X_test_full.index)\n",
    "\n",
    "X_train_reduced_scaled = pd.DataFrame(X_train_reduced_scaled, \n",
    "                                       columns=X_train_reduced.columns,\n",
    "                                       index=X_train_reduced.index)\n",
    "X_test_reduced_scaled = pd.DataFrame(X_test_reduced_scaled, \n",
    "                                      columns=X_test_reduced.columns,\n",
    "                                      index=X_test_reduced.index)\n",
    "\n",
    "print(\"‚úì Feature scaling complete\\n\")\n",
    "print(\"Scaled features have mean=0, std=1\")\n",
    "print(f\"\\nExample - Train set (scaled):\")\n",
    "print(f\"  Mean: {X_train_full_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std:  {X_train_full_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Models Need Scaling?\n",
    "\n",
    "**Require scaling:**\n",
    "- ‚úÖ Logistic Regression (gradient-based optimization)\n",
    "- ‚úÖ Neural Networks (gradient-based)\n",
    "- ‚úÖ GLM (for numerical stability)\n",
    "\n",
    "**Don't require scaling:**\n",
    "- ‚ùå Random Forest (tree-based, scale-invariant)\n",
    "- ‚ùå XGBoost (tree-based)\n",
    "- ‚ùå LightGBM (tree-based)\n",
    "\n",
    "**Strategy:**\n",
    "- We keep both scaled and unscaled versions\n",
    "- Use appropriate version for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Prepared Data\n",
    "\n",
    "Save splits for use in modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory\n",
    "import os\n",
    "os.makedirs('../../data/processed/splits', exist_ok=True)\n",
    "\n",
    "# Save unscaled (for tree models)\n",
    "X_train_full.to_parquet('../../data/processed/splits/X_train_full.parquet')\n",
    "X_test_full.to_parquet('../../data/processed/splits/X_test_full.parquet')\n",
    "X_train_reduced.to_parquet('../../data/processed/splits/X_train_reduced.parquet')\n",
    "X_test_reduced.to_parquet('../../data/processed/splits/X_test_reduced.parquet')\n",
    "\n",
    "# Save scaled (for linear models)\n",
    "X_train_full_scaled.to_parquet('../../data/processed/splits/X_train_full_scaled.parquet')\n",
    "X_test_full_scaled.to_parquet('../../data/processed/splits/X_test_full_scaled.parquet')\n",
    "X_train_reduced_scaled.to_parquet('../../data/processed/splits/X_train_reduced_scaled.parquet')\n",
    "X_test_reduced_scaled.to_parquet('../../data/processed/splits/X_test_reduced_scaled.parquet')\n",
    "\n",
    "# Save targets\n",
    "y_train.to_frame('y').to_parquet('../../data/processed/splits/y_train.parquet')\n",
    "y_test.to_frame('y').to_parquet('../../data/processed/splits/y_test.parquet')\n",
    "\n",
    "print(\"‚úì Saved all splits to: data/processed/splits/\\n\")\n",
    "print(\"Files created:\")\n",
    "print(\"  ‚Ä¢ X_train_full.parquet (unscaled, for RF)\")\n",
    "print(\"  ‚Ä¢ X_train_reduced.parquet (unscaled, correlation pruned)\")\n",
    "print(\"  ‚Ä¢ X_train_full_scaled.parquet (scaled, for Logit)\")\n",
    "print(\"  ‚Ä¢ X_train_reduced_scaled.parquet (scaled, for GLM)\")\n",
    "print(\"  ‚Ä¢ ... and corresponding test sets\")\n",
    "print(\"  ‚Ä¢ y_train.parquet, y_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "summary = {\n",
    "    'Total Samples': len(X_full),\n",
    "    'Train Samples': len(X_train_full),\n",
    "    'Test Samples': len(X_test_full),\n",
    "    'Train/Test Split': f\"{(1-TEST_SIZE)*100:.0f}/{TEST_SIZE*100:.0f}\",\n",
    "    'Random Seed': RANDOM_STATE,\n",
    "    'Stratified': 'Yes',\n",
    "    'Features (Full)': len(X_full.columns),\n",
    "    'Features (Reduced)': len(X_reduced.columns),\n",
    "    'Bankruptcy Rate (Train)': f\"{y_train.mean():.2%}\",\n",
    "    'Bankruptcy Rate (Test)': f\"{y_test.mean():.2%}\",\n",
    "    'Scaling Applied': 'Yes (StandardScaler)',\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame.from_dict(summary, orient='index', columns=['Value'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "display(summary_df)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Did:\n",
    "\n",
    "1. ‚úÖ **Quality Check** - Verified preprocessed data\n",
    "2. ‚úÖ **Train/Test Splits** - Stratified 80/20 split (seed=42)\n",
    "3. ‚úÖ **Feature Scaling** - StandardScaler (fit on train only)\n",
    "4. ‚úÖ **Saved Splits** - Ready for modeling\n",
    "\n",
    "### Data Ready for Modeling:\n",
    "\n",
    "**For Random Forest:**\n",
    "- Use: `X_train_full.parquet` (unscaled)\n",
    "- Features: All 66 features\n",
    "- Handles multicollinearity\n",
    "\n",
    "**For Logistic Regression:**\n",
    "- Use: `X_train_reduced_scaled.parquet` (scaled)\n",
    "- Features: Correlation-pruned\n",
    "- Prevents multicollinearity issues\n",
    "\n",
    "**For GLM:**\n",
    "- Use: `X_train_reduced_scaled.parquet` (scaled)\n",
    "- Same as Logistic (linear model)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Baseline Models** (`04_baseline_models.ipynb`)\n",
    "   - Train Logistic Regression\n",
    "   - Train Random Forest\n",
    "   - Train GLM\n",
    "   - Compare performance\n",
    "\n",
    "2. **Advanced Models** (`05_advanced_models.ipynb`)\n",
    "   - XGBoost, LightGBM, Neural Networks\n",
    "   - Push toward higher accuracy\n",
    "\n",
    "3. **Calibration** (`06_model_calibration.ipynb`)\n",
    "   - Probability reliability\n",
    "   - Threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úì Data preparation complete!\")\n",
    "print(\"\\nüìä Ready for modeling:\")\n",
    "print(f\"  ‚Ä¢ {len(X_train_full):,} training samples\")\n",
    "print(f\"  ‚Ä¢ {len(X_test_full):,} test samples\")\n",
    "print(f\"  ‚Ä¢ {len(X_full.columns)} features (full)\")\n",
    "print(f\"  ‚Ä¢ {len(X_reduced.columns)} features (reduced)\")\n",
    "print(f\"\\nNext: 04_baseline_models.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
