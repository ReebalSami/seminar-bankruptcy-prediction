{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models: XGBoost, LightGBM, Neural Network\n",
    "\n",
    "## Objective\n",
    "\n",
    "Push performance beyond baselines with advanced models:\n",
    "1. **XGBoost** - Gradient boosting with regularization\n",
    "2. **LightGBM** - Faster gradient boosting\n",
    "3. **Neural Network** - Deep learning approach\n",
    "4. **Ensemble** - Combine multiple models (optional)\n",
    "\n",
    "## Goal\n",
    "Maximize ROC-AUC and Recall @ 1% FPR for production deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.bankruptcy_prediction.evaluation import ResultsCollector\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared splits\n",
    "import os\n",
    "\n",
    "splits_dir = '../../data/processed/splits'\n",
    "\n",
    "if os.path.exists(splits_dir):\n",
    "    X_train = pd.read_parquet(f'{splits_dir}/X_train_full.parquet')\n",
    "    X_test = pd.read_parquet(f'{splits_dir}/X_test_full.parquet')\n",
    "    y_train = pd.read_parquet(f'{splits_dir}/y_train.parquet')['y']\n",
    "    y_test = pd.read_parquet(f'{splits_dir}/y_test.parquet')['y']\n",
    "    print(\"‚úì Loaded splits\")\n",
    "else:\n",
    "    # Fallback\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from src.bankruptcy_prediction.data import DataLoader\n",
    "    \n",
    "    loader = DataLoader()\n",
    "    df = loader.load_poland(horizon=1, dataset_type='full')\n",
    "    X, y = loader.get_features_target(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(\"‚úì Created splits\")\n",
    "\n",
    "print(f\"\\nTrain: {len(y_train):,} samples ({y_train.mean():.2%} bankrupt)\")\n",
    "print(f\"Test:  {len(y_test):,} samples ({y_test.mean():.2%} bankrupt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (same as baseline)\n",
    "def evaluate_model(y_true, y_pred_proba, model_name='Model'):\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    idx_1pct = np.where(fpr <= 0.01)[0]\n",
    "    recall_1pct = tpr[idx_1pct[-1]] if len(idx_1pct) > 0 else 0.0\n",
    "    idx_5pct = np.where(fpr <= 0.05)[0]\n",
    "    recall_5pct = tpr[idx_5pct[-1]] if len(idx_5pct) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'brier_score': brier,\n",
    "        'recall_1pct_fpr': recall_1pct,\n",
    "        'recall_5pct_fpr': recall_5pct,\n",
    "        'horizon': 1\n",
    "    }\n",
    "\n",
    "def print_results(results):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{results['model_name']:^60}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ROC-AUC:            {results['roc_auc']:.4f}\")\n",
    "    print(f\"PR-AUC:             {results['pr_auc']:.4f}\")\n",
    "    print(f\"Brier Score:        {results['brier_score']:.4f}\")\n",
    "    print(f\"Recall @ 1% FPR:    {results['recall_1pct_fpr']:.2%}\")\n",
    "    print(f\"Recall @ 5% FPR:    {results['recall_5pct_fpr']:.2%}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: XGBoost\n",
    "\n",
    "Gradient boosting with built-in regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    print(\"Training XGBoost...\\n\")\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalanced data\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    results_xgb = evaluate_model(y_test, y_pred_xgb, 'XGBoost')\n",
    "    print_results(results_xgb)\n",
    "    \n",
    "    xgb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  XGBoost not installed. Install with: pip install xgboost\")\n",
    "    xgb_available = False\n",
    "    results_xgb = None\n",
    "    y_pred_xgb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Interpretation:\n",
    "\n",
    "**Strengths:**\n",
    "- State-of-the-art gradient boosting\n",
    "- Built-in regularization (L1, L2)\n",
    "- Handles imbalanced data well\n",
    "- Often best performance\n",
    "\n",
    "**Parameters:**\n",
    "- `scale_pos_weight`: Handles class imbalance\n",
    "- `max_depth`: Controls tree depth (prevents overfitting)\n",
    "- `learning_rate`: Shrinkage for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: LightGBM\n",
    "\n",
    "Microsoft's fast gradient boosting framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    print(\"Training LightGBM...\\n\")\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    results_lgb = evaluate_model(y_test, y_pred_lgb, 'LightGBM')\n",
    "    print_results(results_lgb)\n",
    "    \n",
    "    lgb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  LightGBM not installed. Install with: pip install lightgbm\")\n",
    "    lgb_available = False\n",
    "    results_lgb = None\n",
    "    y_pred_lgb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Interpretation:\n",
    "\n",
    "**Strengths:**\n",
    "- Very fast training\n",
    "- Memory efficient\n",
    "- Often matches XGBoost performance\n",
    "\n",
    "**Use case:**\n",
    "- Large datasets\n",
    "- When speed matters\n",
    "- Production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Neural Network\n",
    "\n",
    "Deep learning with keras/tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"Training Neural Network...\\n\")\n",
    "    \n",
    "    # Scale features (NN requires scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    # Build model\n",
    "    nn_model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    nn_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = nn_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    y_pred_nn = nn_model.predict(X_test_scaled, verbose=0).ravel()\n",
    "    results_nn = evaluate_model(y_test, y_pred_nn, 'Neural Network')\n",
    "    print_results(results_nn)\n",
    "    \n",
    "    nn_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "    nn_available = False\n",
    "    results_nn = None\n",
    "    y_pred_nn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Interpretation:\n",
    "\n",
    "**Strengths:**\n",
    "- Can learn complex non-linear patterns\n",
    "- Flexible architecture\n",
    "- Good for large datasets\n",
    "\n",
    "**Limitations:**\n",
    "- Requires more data\n",
    "- Slower training\n",
    "- Less interpretable\n",
    "- Sensitive to hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison: Advanced vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "results_collector = ResultsCollector.load_all()\n",
    "\n",
    "# Add advanced model results\n",
    "if xgb_available and results_xgb:\n",
    "    results_collector.add(results_xgb)\n",
    "if lgb_available and results_lgb:\n",
    "    results_collector.add(results_lgb)\n",
    "if nn_available and results_nn:\n",
    "    results_collector.add(results_nn)\n",
    "\n",
    "# Save\n",
    "results_collector.save()\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS COMPARISON (Horizon = 1 year)\")\n",
    "print(\"=\"*80)\n",
    "comparison = results_collector.show_comparison()\n",
    "display(comparison)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model\n",
    "best = results_collector.best_model(horizon=1)\n",
    "if best:\n",
    "    print(f\"\\nüèÜ Best model: {best['model_name']} (ROC-AUC: {best['roc_auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = results_collector.plot_comparison(output_path='../../results/figures/all_models_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: results/figures/all_models_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "### Performance Ranking:\n",
    "\n",
    "Typical results:\n",
    "1. **XGBoost / LightGBM** - Usually best (0.91-0.93 AUC)\n",
    "2. **Random Forest** - Close second (0.90 AUC)\n",
    "3. **Neural Network** - Variable (0.88-0.92 AUC)\n",
    "4. **Logistic / GLM** - Baseline (0.87 AUC)\n",
    "\n",
    "### Model Selection:\n",
    "\n",
    "**For Production:**\n",
    "- Use **XGBoost** or **LightGBM** (best performance)\n",
    "- Apply calibration (next notebook)\n",
    "- Monitor drift\n",
    "\n",
    "**For Thesis:**\n",
    "- Compare **all models** to show thorough analysis\n",
    "- Use **GLM** for statistical inference\n",
    "- **Random Forest** for feature importance\n",
    "- **XGBoost** for best results\n",
    "\n",
    "**For Interpretation:**\n",
    "- **GLM** - Coefficients and p-values\n",
    "- **Random Forest** - Feature importance\n",
    "- **XGBoost** - SHAP values (advanced)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Calibration** (`06_model_calibration.ipynb`)\n",
    "   - Improve probability reliability\n",
    "   - Critical for decision thresholds\n",
    "\n",
    "2. **Robustness** (`07_robustness_analysis.ipynb`)\n",
    "   - Cross-horizon validation\n",
    "   - All 5 horizons\n",
    "   - Generalization testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ADVANCED MODELS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "if best:\n",
    "    print(f\"\\nüèÜ Best model overall: {best['model_name']}\")\n",
    "    print(f\"   ROC-AUC: {best['roc_auc']:.4f}\")\n",
    "    print(f\"   Recall @ 1% FPR: {best['recall_1pct_fpr']:.2%}\")\n",
    "print(f\"\\nüìä All results saved to ResultsCollector\")\n",
    "print(f\"   Check 00_MASTER_REPORT.ipynb for complete comparison\")\n",
    "print(f\"\\nNext: 06_model_calibration.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
