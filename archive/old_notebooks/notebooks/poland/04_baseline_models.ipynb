{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models: Logistic, Random Forest, GLM\n",
    "\n",
    "## Objective\n",
    "\n",
    "Train and evaluate baseline models:\n",
    "1. **Logistic Regression** - Linear baseline with L2 regularization\n",
    "2. **Random Forest** - Non-linear tree ensemble\n",
    "3. **GLM Binomial** - Statistical model with interpretable coefficients\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **ROC-AUC** - Overall discriminative power\n",
    "- **PR-AUC** - Performance on imbalanced data\n",
    "- **Recall @ 1% FPR** - Early warning sensitivity\n",
    "- **Recall @ 5% FPR** - Alternative threshold\n",
    "- **Brier Score** - Probability calibration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    roc_curve, precision_recall_curve, classification_report\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.bankruptcy_prediction.data import DataLoader, MetadataParser\n",
    "from src.bankruptcy_prediction.evaluation import ResultsCollector\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared splits\n",
    "import os\n",
    "\n",
    "splits_dir = '../../data/processed/splits'\n",
    "\n",
    "if os.path.exists(splits_dir):\n",
    "    print(\"Loading prepared splits from 03_data_preparation.ipynb...\\n\")\n",
    "    \n",
    "    X_train_full = pd.read_parquet(f'{splits_dir}/X_train_full.parquet')\n",
    "    X_test_full = pd.read_parquet(f'{splits_dir}/X_test_full.parquet')\n",
    "    X_train_reduced_scaled = pd.read_parquet(f'{splits_dir}/X_train_reduced_scaled.parquet')\n",
    "    X_test_reduced_scaled = pd.read_parquet(f'{splits_dir}/X_test_reduced_scaled.parquet')\n",
    "    y_train = pd.read_parquet(f'{splits_dir}/y_train.parquet')['y']\n",
    "    y_test = pd.read_parquet(f'{splits_dir}/y_test.parquet')['y']\n",
    "    \n",
    "    print(\"‚úì Loaded splits\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Splits not found. Run 03_data_preparation.ipynb first.\")\n",
    "    print(\"   Creating splits now...\\n\")\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    loader = DataLoader()\n",
    "    df_full = loader.load_poland(horizon=1, dataset_type='full')\n",
    "    df_reduced = loader.load_poland(horizon=1, dataset_type='reduced')\n",
    "    \n",
    "    X_full, y = loader.get_features_target(df_full)\n",
    "    X_reduced, _ = loader.get_features_target(df_reduced)\n",
    "    \n",
    "    X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "        X_full, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train_reduced, X_test_reduced, _, _ = train_test_split(\n",
    "        X_reduced, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_reduced_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_reduced),\n",
    "        columns=X_train_reduced.columns,\n",
    "        index=X_train_reduced.index\n",
    "    )\n",
    "    X_test_reduced_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test_reduced),\n",
    "        columns=X_test_reduced.columns,\n",
    "        index=X_test_reduced.index\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created splits\")\n",
    "\n",
    "print(f\"\\nTrain: {len(y_train):,} samples ({y_train.mean():.2%} bankrupt)\")\n",
    "print(f\"Test:  {len(y_test):,} samples ({y_test.mean():.2%} bankrupt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred_proba, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \n",
    "    Returns dict with all metrics.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    \n",
    "    # Recall at specific FPR thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    \n",
    "    # Recall @ 1% FPR\n",
    "    idx_1pct = np.where(fpr <= 0.01)[0]\n",
    "    recall_1pct = tpr[idx_1pct[-1]] if len(idx_1pct) > 0 else 0.0\n",
    "    \n",
    "    # Recall @ 5% FPR\n",
    "    idx_5pct = np.where(fpr <= 0.05)[0]\n",
    "    recall_5pct = tpr[idx_5pct[-1]] if len(idx_5pct) > 0 else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'brier_score': brier,\n",
    "        'recall_1pct_fpr': recall_1pct,\n",
    "        'recall_5pct_fpr': recall_5pct,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"Pretty print evaluation results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{results['model_name']:^60}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ROC-AUC:            {results['roc_auc']:.4f}\")\n",
    "    print(f\"PR-AUC:             {results['pr_auc']:.4f}\")\n",
    "    print(f\"Brier Score:        {results['brier_score']:.4f} (lower is better)\")\n",
    "    print(f\"Recall @ 1% FPR:    {results['recall_1pct_fpr']:.2%}\")\n",
    "    print(f\"Recall @ 5% FPR:    {results['recall_5pct_fpr']:.2%}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression\n",
    "\n",
    "Linear baseline with L2 regularization and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression with GridSearchCV...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_logit = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "# Create model with class weights\n",
    "logit = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Grid search with stratified CV\n",
    "gs_logit = GridSearchCV(\n",
    "    logit,\n",
    "    param_grid_logit,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train (use scaled, reduced features)\n",
    "gs_logit.fit(X_train_reduced_scaled, y_train)\n",
    "\n",
    "print(f\"‚úì Training complete\")\n",
    "print(f\"  Best parameters: {gs_logit.best_params_}\")\n",
    "print(f\"  Best CV ROC-AUC: {gs_logit.best_score_:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_logit = gs_logit.predict_proba(X_test_reduced_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "results_logit = evaluate_model(y_test, y_pred_logit, 'Logistic Regression')\n",
    "print_results(results_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Interpretation:\n",
    "\n",
    "**Strengths:**\n",
    "- Fast training\n",
    "- Interpretable coefficients\n",
    "- Good baseline performance\n",
    "\n",
    "**Limitations:**\n",
    "- Assumes linear relationships\n",
    "- May underfit complex patterns\n",
    "- Sensitive to feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n",
    "\n",
    "Non-linear ensemble model with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest with GridSearchCV...\\n\")\n",
    "\n",
    "# Define parameter grid (smaller for speed)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Create model with class weights\n",
    "rf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Grid search (fewer CV folds for speed)\n",
    "gs_rf = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid_rf,\n",
    "    cv=3,  # Reduced for speed\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train (use unscaled, full features - RF doesn't need scaling)\n",
    "gs_rf.fit(X_train_full, y_train)\n",
    "\n",
    "print(f\"\\n‚úì Training complete\")\n",
    "print(f\"  Best parameters: {gs_rf.best_params_}\")\n",
    "print(f\"  Best CV ROC-AUC: {gs_rf.best_score_:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = gs_rf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "results_rf = evaluate_model(y_test, y_pred_rf, 'Random Forest')\n",
    "print_results(results_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Interpretation:\n",
    "\n",
    "**Strengths:**\n",
    "- Captures non-linear relationships\n",
    "- Handles multicollinearity\n",
    "- Feature importance available\n",
    "- Usually best performance\n",
    "\n",
    "**Limitations:**\n",
    "- Less interpretable\n",
    "- Longer training time\n",
    "- May overfit without tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: GLM (Statsmodels)\n",
    "\n",
    "Statistical model with standard errors and p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training GLM Binomial...\\n\")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant\n",
    "X_train_glm = sm.add_constant(X_train_reduced_scaled)\n",
    "X_test_glm = sm.add_constant(X_test_reduced_scaled)\n",
    "\n",
    "# Fit GLM with binomial family\n",
    "glm_model = sm.GLM(\n",
    "    y_train,\n",
    "    X_train_glm,\n",
    "    family=sm.families.Binomial()\n",
    ")\n",
    "\n",
    "glm_result = glm_model.fit()\n",
    "\n",
    "print(\"‚úì Training complete\\n\")\n",
    "print(\"Model Summary:\")\n",
    "print(glm_result.summary().tables[0])\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_glm = glm_result.predict(X_test_glm)\n",
    "\n",
    "# Evaluate\n",
    "results_glm = evaluate_model(y_test, y_pred_glm, 'GLM Binomial')\n",
    "print_results(results_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM Interpretation:\n",
    "\n",
    "**Strengths:**\n",
    "- Statistical inference (p-values, confidence intervals)\n",
    "- Interpretable odds ratios\n",
    "- Similar performance to Logistic\n",
    "\n",
    "**Use cases:**\n",
    "- When statistical significance needed\n",
    "- For thesis - connect to financial theory\n",
    "- Publication-ready coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame([results_logit, results_rf, results_glm])\n",
    "comparison = comparison.sort_values('roc_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON (Horizon = 1 year)\")\n",
    "print(\"=\"*80)\n",
    "display(comparison[['model_name', 'roc_auc', 'pr_auc', 'recall_1pct_fpr', 'recall_5pct_fpr', 'brier_score']])\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best model\n",
    "best_model = comparison.iloc[0]\n",
    "print(f\"\\nüèÜ Best model: {best_model['model_name']} (ROC-AUC: {best_model['roc_auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: ROC & PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC curves\n",
    "for name, y_pred in [('Logistic', y_pred_logit), ('Random Forest', y_pred_rf), ('GLM', y_pred_glm)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    ax1.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', linewidth=2)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "ax1.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "ax1.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "ax1.set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall curves\n",
    "for name, y_pred in [('Logistic', y_pred_logit), ('Random Forest', y_pred_rf), ('GLM', y_pred_glm)]:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "    ap = average_precision_score(y_test, y_pred)\n",
    "    ax2.plot(recall, precision, label=f'{name} (AP={ap:.3f})', linewidth=2)\n",
    "\n",
    "baseline = y_test.mean()\n",
    "ax2.axhline(baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})', linewidth=1)\n",
    "ax2.set_xlabel('Recall', fontweight='bold')\n",
    "ax2.set_ylabel('Precision', fontweight='bold')\n",
    "ax2.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/baseline_models_roc_pr.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: results/figures/baseline_models_roc_pr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to ResultsCollector\n",
    "\n",
    "These will appear automatically in the Master Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results collector\n",
    "results_collector = ResultsCollector()\n",
    "\n",
    "# Add horizon information\n",
    "for result in [results_logit, results_rf, results_glm]:\n",
    "    result['horizon'] = 1\n",
    "    results_collector.add(result)\n",
    "\n",
    "# Save\n",
    "results_collector.save()\n",
    "\n",
    "print(\"\\n‚úì Results saved to: results/models/all_results.csv\")\n",
    "print(\"  These will appear in 00_MASTER_REPORT.ipynb automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Interpretation\n",
    "\n",
    "### Performance Summary:\n",
    "\n",
    "**Random Forest typically wins:**\n",
    "- Best ROC-AUC (~0.90)\n",
    "- Best Recall @ 1% FPR (~57%)\n",
    "- Captures non-linear patterns\n",
    "\n",
    "**Logistic Regression:**\n",
    "- Good baseline (~0.87 AUC)\n",
    "- Fast and interpretable\n",
    "- May underfit complex relationships\n",
    "\n",
    "**GLM:**\n",
    "- Similar to Logistic\n",
    "- Statistical inference available\n",
    "- Good for thesis (p-values, CIs)\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Non-linearity matters** - RF outperforms linear models\n",
    "2. **Imbalanced data handled** - Class weights effective\n",
    "3. **Recall @ 1% FPR** - Critical metric for early warning systems\n",
    "4. **All models beat random** - Financial ratios are predictive\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Advanced Models** (`05_advanced_models.ipynb`)\n",
    "   - XGBoost, LightGBM, Neural Networks\n",
    "   - Push performance higher\n",
    "\n",
    "2. **Calibration** (`06_model_calibration.ipynb`)\n",
    "   - Improve probability reliability\n",
    "   - Threshold optimization\n",
    "\n",
    "3. **Robustness** (`07_robustness_analysis.ipynb`)\n",
    "   - Cross-horizon validation\n",
    "   - All 5 horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì BASELINE MODELS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best model: {best_model['model_name']}\")\n",
    "print(f\"   ROC-AUC: {best_model['roc_auc']:.4f}\")\n",
    "print(f\"   Recall @ 1% FPR: {best_model['recall_1pct_fpr']:.2%}\")\n",
    "print(f\"\\nüìä Results saved to ResultsCollector\")\n",
    "print(f\"   Check 00_MASTER_REPORT.ipynb to see aggregated comparison\")\n",
    "print(f\"\\nNext: 05_advanced_models.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
