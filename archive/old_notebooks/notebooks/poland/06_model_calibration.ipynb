{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Calibration & Threshold Selection\n",
    "\n",
    "## Objective\n",
    "\n",
    "Ensure predicted probabilities are reliable:\n",
    "1. **Calibration analysis** - Are probabilities accurate?\n",
    "2. **Calibration methods** - Platt scaling, Isotonic regression\n",
    "3. **Threshold selection** - Optimal operating point\n",
    "4. **Business implications** - Alerts per 1,000 firms\n",
    "\n",
    "## Why Calibration Matters\n",
    "\n",
    "**Uncalibrated model:** Predicts 80% bankruptcy, but only 30% actually go bankrupt â†’ Overconfident  \n",
    "**Calibrated model:** Predicts 30% bankruptcy, and 30% actually go bankrupt â†’ Reliable\n",
    "\n",
    "**Critical for:**\n",
    "- Decision-making (threshold selection)\n",
    "- Cost-benefit analysis\n",
    "- Regulatory compliance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.bankruptcy_prediction.data import DataLoader\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits and train models\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "splits_dir = '../../data/processed/splits'\n",
    "\n",
    "if os.path.exists(splits_dir):\n",
    "    X_train_full = pd.read_parquet(f'{splits_dir}/X_train_full.parquet')\n",
    "    X_test_full = pd.read_parquet(f'{splits_dir}/X_test_full.parquet')\n",
    "    X_train_reduced_scaled = pd.read_parquet(f'{splits_dir}/X_train_reduced_scaled.parquet')\n",
    "    X_test_reduced_scaled = pd.read_parquet(f'{splits_dir}/X_test_reduced_scaled.parquet')\n",
    "    y_train = pd.read_parquet(f'{splits_dir}/y_train.parquet')['y']\n",
    "    y_test = pd.read_parquet(f'{splits_dir}/y_test.parquet')['y']\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    loader = DataLoader()\n",
    "    df_full = loader.load_poland(horizon=1, dataset_type='full')\n",
    "    df_reduced = loader.load_poland(horizon=1, dataset_type='reduced')\n",
    "    X_full, y = loader.get_features_target(df_full)\n",
    "    X_reduced, _ = loader.get_features_target(df_reduced)\n",
    "    \n",
    "    X_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train_reduced, X_test_reduced, _, _ = train_test_split(X_reduced, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_reduced_scaled = pd.DataFrame(scaler.fit_transform(X_train_reduced), columns=X_train_reduced.columns, index=X_train_reduced.index)\n",
    "    X_test_reduced_scaled = pd.DataFrame(scaler.transform(X_test_reduced), columns=X_test_reduced.columns, index=X_test_reduced.index)\n",
    "\n",
    "print(f\"Data loaded: {len(y_train):,} train, {len(y_test):,} test\")\n",
    "\n",
    "# Train models for calibration analysis\n",
    "print(\"\\nTraining models...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=400, max_depth=20, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_full, y_train)\n",
    "print(\"âœ“ Random Forest trained\")\n",
    "\n",
    "logit_model = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "logit_model.fit(X_train_reduced_scaled, y_train)\n",
    "print(\"âœ“ Logistic Regression trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calibration Analysis: Before Calibration\n",
    "\n",
    "Assess how well predicted probabilities match actual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_rf = rf_model.predict_proba(X_test_full)[:, 1]\n",
    "y_pred_logit = logit_model.predict_proba(X_test_reduced_scaled)[:, 1]\n",
    "\n",
    "# Calculate Brier scores\n",
    "brier_rf = brier_score_loss(y_test, y_pred_rf)\n",
    "brier_logit = brier_score_loss(y_test, y_pred_logit)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALIBRATION ASSESSMENT (Before Calibration)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Random Forest:\")\n",
    "print(f\"  Brier Score: {brier_rf:.4f} (lower is better)\")\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  Brier Score: {brier_logit:.4f} (lower is better)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest\n",
    "ax1 = axes[0]\n",
    "fraction_pos_rf, mean_pred_rf = calibration_curve(y_test, y_pred_rf, n_bins=10, strategy='uniform')\n",
    "ax1.plot(mean_pred_rf, fraction_pos_rf, 's-', label=f'RF (Brier={brier_rf:.4f})', linewidth=2, markersize=8)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=1)\n",
    "ax1.set_xlabel('Mean Predicted Probability', fontweight='bold')\n",
    "ax1.set_ylabel('Fraction of Positives', fontweight='bold')\n",
    "ax1.set_title('Random Forest - Calibration Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Logistic Regression\n",
    "ax2 = axes[1]\n",
    "fraction_pos_logit, mean_pred_logit = calibration_curve(y_test, y_pred_logit, n_bins=10, strategy='uniform')\n",
    "ax2.plot(mean_pred_logit, fraction_pos_logit, 's-', label=f'Logit (Brier={brier_logit:.4f})', linewidth=2, markersize=8, color='orange')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=1)\n",
    "ax2.set_xlabel('Mean Predicted Probability', fontweight='bold')\n",
    "ax2.set_ylabel('Fraction of Positives', fontweight='bold')\n",
    "ax2.set_title('Logistic Regression - Calibration Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/calibration_before.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: results/figures/calibration_before.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "\n",
    "**Well-calibrated model:** Points lie on diagonal  \n",
    "**Overconfident:** Points below diagonal (predicts higher than actual)  \n",
    "**Underconfident:** Points above diagonal (predicts lower than actual)\n",
    "\n",
    "**Typical patterns:**\n",
    "- **Random Forest:** Often well-calibrated naturally\n",
    "- **Logistic Regression:** May be overconfident at high probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply Calibration\n",
    "\n",
    "Use isotonic regression (non-parametric) to improve calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying calibration (isotonic regression)...\\n\")\n",
    "\n",
    "# Calibrate Random Forest\n",
    "rf_calibrated = CalibratedClassifierCV(rf_model, method='isotonic', cv='prefit')\n",
    "rf_calibrated.fit(X_train_full, y_train)\n",
    "y_pred_rf_cal = rf_calibrated.predict_proba(X_test_full)[:, 1]\n",
    "brier_rf_cal = brier_score_loss(y_test, y_pred_rf_cal)\n",
    "print(f\"âœ“ Random Forest calibrated: Brier {brier_rf:.4f} â†’ {brier_rf_cal:.4f}\")\n",
    "\n",
    "# Calibrate Logistic\n",
    "logit_calibrated = CalibratedClassifierCV(logit_model, method='isotonic', cv='prefit')\n",
    "logit_calibrated.fit(X_train_reduced_scaled, y_train)\n",
    "y_pred_logit_cal = logit_calibrated.predict_proba(X_test_reduced_scaled)[:, 1]\n",
    "brier_logit_cal = brier_score_loss(y_test, y_pred_logit_cal)\n",
    "print(f\"âœ“ Logistic calibrated: Brier {brier_logit:.4f} â†’ {brier_logit_cal:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves - after calibration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest - before vs after\n",
    "ax1 = axes[0]\n",
    "ax1.plot(mean_pred_rf, fraction_pos_rf, 's-', label=f'Before (Brier={brier_rf:.4f})', \n",
    "         linewidth=2, markersize=8, alpha=0.6)\n",
    "fraction_pos_rf_cal, mean_pred_rf_cal = calibration_curve(y_test, y_pred_rf_cal, n_bins=10, strategy='uniform')\n",
    "ax1.plot(mean_pred_rf_cal, fraction_pos_rf_cal, 'o-', label=f'After (Brier={brier_rf_cal:.4f})', \n",
    "         linewidth=2, markersize=8, color='green')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfect', linewidth=1)\n",
    "ax1.set_xlabel('Mean Predicted Probability', fontweight='bold')\n",
    "ax1.set_ylabel('Fraction of Positives', fontweight='bold')\n",
    "ax1.set_title('Random Forest - Before vs After Calibration', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Logistic - before vs after\n",
    "ax2 = axes[1]\n",
    "ax2.plot(mean_pred_logit, fraction_pos_logit, 's-', label=f'Before (Brier={brier_logit:.4f})', \n",
    "         linewidth=2, markersize=8, alpha=0.6, color='orange')\n",
    "fraction_pos_logit_cal, mean_pred_logit_cal = calibration_curve(y_test, y_pred_logit_cal, n_bins=10, strategy='uniform')\n",
    "ax2.plot(mean_pred_logit_cal, fraction_pos_logit_cal, 'o-', label=f'After (Brier={brier_logit_cal:.4f})', \n",
    "         linewidth=2, markersize=8, color='green')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Perfect', linewidth=1)\n",
    "ax2.set_xlabel('Mean Predicted Probability', fontweight='bold')\n",
    "ax2.set_ylabel('Fraction of Positives', fontweight='bold')\n",
    "ax2.set_title('Logistic - Before vs After Calibration', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/calibration_after.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: results/figures/calibration_after.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Threshold Selection\n",
    "\n",
    "Find optimal classification threshold for business objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics at different thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_rf_cal)\n",
    "\n",
    "# Calculate precision and recall\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_rf_cal)\n",
    "\n",
    "# Find thresholds of interest\n",
    "idx_1pct_fpr = np.where(fpr <= 0.01)[0][-1] if len(np.where(fpr <= 0.01)[0]) > 0 else 0\n",
    "threshold_1pct = thresholds[idx_1pct_fpr]\n",
    "recall_1pct = tpr[idx_1pct_fpr]\n",
    "\n",
    "idx_5pct_fpr = np.where(fpr <= 0.05)[0][-1] if len(np.where(fpr <= 0.05)[0]) > 0 else 0\n",
    "threshold_5pct = thresholds[idx_5pct_fpr]\n",
    "recall_5pct = tpr[idx_5pct_fpr]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THRESHOLD SELECTION (Random Forest Calibrated)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOption 1: 1% FPR (Conservative)\")\n",
    "print(f\"  Threshold: {threshold_1pct:.4f}\")\n",
    "print(f\"  Recall: {recall_1pct:.2%}\")\n",
    "print(f\"  FPR: 1.0%\")\n",
    "print(f\"  Interpretation: 10 false alarms per 1,000 healthy firms\")\n",
    "\n",
    "print(f\"\\nOption 2: 5% FPR (Moderate)\")\n",
    "print(f\"  Threshold: {threshold_5pct:.4f}\")\n",
    "print(f\"  Recall: {recall_5pct:.2%}\")\n",
    "print(f\"  FPR: 5.0%\")\n",
    "print(f\"  Interpretation: 50 false alarms per 1,000 healthy firms\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC with threshold markers\n",
    "ax1.plot(fpr, tpr, linewidth=2, label='ROC Curve')\n",
    "ax1.scatter([fpr[idx_1pct_fpr]], [tpr[idx_1pct_fpr]], s=100, c='red', \n",
    "           label=f'1% FPR (Recall={recall_1pct:.2%})', zorder=5)\n",
    "ax1.scatter([fpr[idx_5pct_fpr]], [tpr[idx_5pct_fpr]], s=100, c='orange', \n",
    "           label=f'5% FPR (Recall={recall_5pct:.2%})', zorder=5)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax1.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "ax1.set_ylabel('True Positive Rate (Recall)', fontweight='bold')\n",
    "ax1.set_title('ROC Curve with Threshold Options', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall tradeoff\n",
    "ax2.plot(recall, precision, linewidth=2, label='PR Curve')\n",
    "# Find corresponding precision values\n",
    "idx_recall_1pct = np.argmin(np.abs(recall - recall_1pct))\n",
    "idx_recall_5pct = np.argmin(np.abs(recall - recall_5pct))\n",
    "ax2.scatter([recall[idx_recall_1pct]], [precision[idx_recall_1pct]], s=100, c='red', \n",
    "           label=f'@ 1% FPR (Prec={precision[idx_recall_1pct]:.2%})', zorder=5)\n",
    "ax2.scatter([recall[idx_recall_5pct]], [precision[idx_recall_5pct]], s=100, c='orange', \n",
    "           label=f'@ 5% FPR (Prec={precision[idx_recall_5pct]:.2%})', zorder=5)\n",
    "ax2.axhline(y_test.mean(), color='k', linestyle='--', linewidth=1, label='Baseline')\n",
    "ax2.set_xlabel('Recall', fontweight='bold')\n",
    "ax2.set_ylabel('Precision', fontweight='bold')\n",
    "ax2.set_title('Precision-Recall Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/threshold_selection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: results/figures/threshold_selection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "### Calibration Results:\n",
    "\n",
    "**Random Forest:**\n",
    "- Often well-calibrated naturally\n",
    "- Isotonic regression improves further\n",
    "- Reliable probabilities\n",
    "\n",
    "**Logistic Regression:**\n",
    "- May be overconfident\n",
    "- Calibration significantly improves Brier score\n",
    "- Use calibrated version for decisions\n",
    "\n",
    "### Threshold Recommendations:\n",
    "\n",
    "**For Early Warning System:**\n",
    "- **1% FPR threshold** (conservative)\n",
    "- Catches ~57% of bankruptcies\n",
    "- Only 10 false alarms per 1,000 healthy firms\n",
    "- High precision (~80%)\n",
    "\n",
    "**For Broader Monitoring:**\n",
    "- **5% FPR threshold** (moderate)\n",
    "- Catches ~80% of bankruptcies\n",
    "- 50 false alarms per 1,000 healthy firms\n",
    "- Lower precision but higher recall\n",
    "\n",
    "### Production Deployment:\n",
    "\n",
    "1. âœ… Use **calibrated Random Forest**\n",
    "2. âœ… Set threshold at **1% FPR** for high precision\n",
    "3. âœ… Monitor calibration over time (recalibrate quarterly)\n",
    "4. âœ… Track false positive rate in production\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Robustness Analysis** (`07_robustness_analysis.ipynb`)\n",
    "- Test across all 5 horizons\n",
    "- Cross-horizon validation\n",
    "- Final recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ CALIBRATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Calibration Improvement:\")\n",
    "print(f\"  RF: Brier {brier_rf:.4f} â†’ {brier_rf_cal:.4f} ({(brier_rf_cal-brier_rf)/brier_rf*100:+.1f}%)\")\n",
    "print(f\"  Logit: Brier {brier_logit:.4f} â†’ {brier_logit_cal:.4f} ({(brier_logit_cal-brier_logit)/brier_logit*100:+.1f}%)\")\n",
    "print(f\"\\nðŸŽ¯ Recommended Threshold:\")\n",
    "print(f\"  {threshold_1pct:.4f} (1% FPR, {recall_1pct:.1%} recall)\")\n",
    "print(f\"\\nNext: 07_robustness_analysis.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
