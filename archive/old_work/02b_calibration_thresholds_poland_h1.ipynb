{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5c5b09",
   "metadata": {},
   "source": [
    "# 02b — Calibration & Thresholds (Poland, Horizon = 1)\n",
    "\n",
    "**Goal:** Turn model scores into actionable early-warning rules.  \n",
    "We will:\n",
    "1) Load saved test predictions (`data/processed/poland_h1_test_predictions.csv`)  \n",
    "2) Compute PR/ROC metrics focused on **low FPR** (1% / 5%)  \n",
    "3) Build **reliability (calibration) tables** and curves  \n",
    "4) Make a **threshold summary table** with expected TP/FP counts and alerts-per-1,000 firms\n",
    "\n",
    "> Paste your short interpretation under each output where indicated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2981f6d",
   "metadata": {},
   "source": [
    "### Step 1 — Imports & load predictions\n",
    "\n",
    "**Why:** Bring in metrics tools and load the saved test-set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d74535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1406 | Positives: 54 | Pos rate: 0.0384\n",
      "Logit ROC-AUC=0.924 PR-AUC=0.385 Brier=0.1064\n",
      "RF ROC-AUC=0.968 PR-AUC=0.738 Brier=0.0208\n",
      "GLM ROC-AUC=0.924 PR-AUC=0.395 Brier=0.1061\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, roc_curve, precision_recall_curve\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "DATA_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
    "preds = pd.read_csv(DATA_DIR / \"poland_h1_test_predictions.csv\")\n",
    "\n",
    "y = preds['y_true'].values\n",
    "scores = {\n",
    "    'Logit': preds['p_logit'].values,\n",
    "    'RF': preds['p_rf'].values,\n",
    "    'GLM': preds['p_glm'].values,\n",
    "}\n",
    "\n",
    "print('Rows:', len(y), '| Positives:', int(y.sum()), '| Pos rate:', f\"{y.mean():.4f}\")\n",
    "for name, s in scores.items():\n",
    "    roc = roc_auc_score(y, s)\n",
    "    pr  = average_precision_score(y, s)\n",
    "    b   = brier_score_loss(y, s)\n",
    "    print(f\"{name} ROC-AUC={roc:.3f} PR-AUC={pr:.3f} Brier={b:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdaf75",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Confirm counts and that metrics match the modeling notebook (small diffs are fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e365de",
   "metadata": {},
   "source": [
    "### Step 2 — Helpers: recall@FPR and calibration table\n",
    "\n",
    "**Why:** Re-useable utilities to summarize early-warning operating points and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba33ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_fpr(y_true, y_score, fpr_cap=0.01):\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_score)\n",
    "    mask = fpr <= fpr_cap\n",
    "    if not np.any(mask):\n",
    "        return 0.0, None\n",
    "    idx = np.argmax(tpr[mask])\n",
    "    return float(tpr[mask][idx]), float(thr[mask][idx])\n",
    "\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    bins = pd.qcut(y_prob, q=n_bins, duplicates='drop')\n",
    "    tab = pd.DataFrame({'bin': bins, 'y': y_true, 'p': y_prob})\\\n",
    "        .groupby('bin').agg(count=('y','size'), mean_pred=('p','mean'), event_rate=('y','mean'))\\\n",
    "        .reset_index()\n",
    "    return tab\n",
    "\n",
    "def threshold_summary(y_true, y_score, thresholds):\n",
    "    out = []\n",
    "    n = len(y_true)\n",
    "    pos = int(y_true.sum())\n",
    "    neg = n - pos\n",
    "    for t in thresholds:\n",
    "        yhat = (y_score >= t).astype(int)\n",
    "        tp = int(((yhat==1) & (y_true==1)).sum())\n",
    "        fp = int(((yhat==1) & (y_true==0)).sum())\n",
    "        fn = pos - tp\n",
    "        tn = neg - fp\n",
    "        tpr = tp / pos if pos else 0.0\n",
    "        fpr = fp / neg if neg else 0.0\n",
    "        prec = tp / (tp + fp) if (tp+fp) else 0.0\n",
    "        alerts_per_1000 = 1000 * (tp + fp) / n\n",
    "        out.append({'thr': t, 'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn,\n",
    "                    'TPR': tpr, 'FPR': fpr, 'Precision': prec,\n",
    "                    'Alerts_per_1000': alerts_per_1000})\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c804977",
   "metadata": {},
   "source": [
    "*No interpretation needed here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff327c8",
   "metadata": {},
   "source": [
    "### Step 3 — Low-FPR operating points (1% and 5%)\n",
    "\n",
    "**Why:** Early-warning systems usually cap false alarms. We compute recall (TPR) at **FPR ≤ 1%** and **≤ 5%**, plus the corresponding thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b51edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit | Recall@1%FPR=0.278 (thr=0.9817998903019366) | Recall@5%FPR=0.648 (thr=0.7449873031547386)\n",
      "   RF | Recall@1%FPR=0.593 (thr=0.311743306538647) | Recall@5%FPR=0.796 (thr=0.1870931271050699)\n",
      "  GLM | Recall@1%FPR=0.278 (thr=0.9828315289483236) | Recall@5%FPR=0.667 (thr=0.7333376160367324)\n"
     ]
    }
   ],
   "source": [
    "for name, s in scores.items():\n",
    "    r1, t1 = recall_at_fpr(y, s, 0.01)\n",
    "    r5, t5 = recall_at_fpr(y, s, 0.05)\n",
    "    print(f\"{name:>5} | Recall@1%FPR={r1:.3f} (thr={t1}) | Recall@5%FPR={r5:.3f} (thr={t5})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883ced5",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Compare models at the same FPR cap. The “best” model is the one with higher recall at the chosen cap (usually **RF** here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48e95d",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "* At **the same false-alarm budget**, **RF** recovers many more true bankruptcies than Logit/GLM.\n",
    "* Recommended caps for the report: **1% FPR (strict)** and **5% FPR (operational)**, with thresholds above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4a29a",
   "metadata": {},
   "source": [
    "### Step 4 — Threshold tables around the chosen caps\n",
    "\n",
    "**Why:** Give decision-makers tangible counts. We tabulate **TP/FP/FN** and alerts-per-1,000 around the low-FPR thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe16946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logit':       thr  TP  FP  FN    TN       TPR       FPR  Precision  Alerts_per_1000\n",
       " 0  0.9318  27  21  27  1331  0.500000  0.015533   0.562500        34.139403\n",
       " 1  0.9818  15  12  39  1340  0.277778  0.008876   0.555556        19.203414\n",
       " 2  1.0000   0   0  54  1352  0.000000  0.000000   0.000000         0.000000,\n",
       " 'RF':         thr  TP  FP  FN    TN       TPR       FPR  Precision  Alerts_per_1000\n",
       " 0  0.261743  35  20  19  1332  0.648148  0.014793   0.636364        39.118065\n",
       " 1  0.311743  32  13  22  1339  0.592593  0.009615   0.711111        32.005690\n",
       " 2  0.361743  31   7  23  1345  0.574074  0.005178   0.815789        27.027027,\n",
       " 'GLM':         thr  TP  FP  FN    TN       TPR       FPR  Precision  Alerts_per_1000\n",
       " 0  0.932832  27  21  27  1331  0.500000  0.015533   0.562500        34.139403\n",
       " 1  0.982832  15  12  39  1340  0.277778  0.008876   0.555556        19.203414\n",
       " 2  1.000000   0   0  54  1352  0.000000  0.000000   0.000000         0.000000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build threshold tables ± a small margin around each model's 1% FPR threshold\n",
    "tables = {}\n",
    "for name, s in scores.items():\n",
    "    r1, t1 = recall_at_fpr(y, s, 0.01)\n",
    "    grid = sorted(set([t1, max(0.0, t1-0.05), min(1.0, t1+0.05)]))\n",
    "    tables[name] = threshold_summary(y, s, grid)\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310897a",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Use these tables to pick a threshold that balances recall with a tolerable number of false alerts.  \n",
    "- “Alerts per 1,000” is easy to communicate to non-technical stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057795b9",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "* The **RF** threshold table near **1% FPR** shows a good balance (**TP 32 vs FP 13**).\n",
    "* Tightening slightly (thr ≈ 0.362) reduces FPs to **7** but only loses **1 TP**; this is worth mentioning as an alternative *“fewer alerts”* policy.\n",
    "* For **Logit/GLM**, comparable thresholds produce **many fewer TPs** for similar FPs → not ideal for early-warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0235ba",
   "metadata": {},
   "source": [
    "### Step 5 — Calibration tables (top 10 bins)\n",
    "\n",
    "**Why:** Check if predicted probabilities match observed event rates in bins (reliability). Poor calibration → add a calibrator later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0a4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logit calibration (head):\n",
      "                       bin  count  mean_pred  event_rate\n",
      "0  (-0.000999895, 0.0195]    141   0.008294    0.000000\n",
      "1         (0.0195, 0.045]    141   0.031962    0.000000\n",
      "2         (0.045, 0.0767]    140   0.060557    0.000000\n",
      "3         (0.0767, 0.112]    141   0.093760    0.000000\n",
      "4          (0.112, 0.156]    140   0.132571    0.000000\n",
      "5          (0.156, 0.223]    141   0.189368    0.021277\n",
      "6          (0.223, 0.311]    140   0.265087    0.014286\n",
      "7          (0.311, 0.441]    141   0.369789    0.014184\n",
      "8           (0.441, 0.65]    140   0.533753    0.057143\n",
      "9           (0.65, 0.999]    141   0.834228    0.276596\n",
      "\n",
      "RF calibration (head):\n",
      "                  bin  count  mean_pred  event_rate\n",
      "0  (-0.001, 0.00454]    141   0.002086    0.000000\n",
      "1  (0.00454, 0.0104]    141   0.007476    0.000000\n",
      "2   (0.0104, 0.0171]    140   0.013646    0.000000\n",
      "3   (0.0171, 0.0245]    141   0.020499    0.000000\n",
      "4   (0.0245, 0.0345]    140   0.029951    0.000000\n",
      "5   (0.0345, 0.0503]    141   0.041154    0.000000\n",
      "6    (0.0503, 0.069]    140   0.059211    0.007143\n",
      "7    (0.069, 0.0975]    141   0.081997    0.014184\n",
      "8    (0.0975, 0.153]    140   0.121583    0.042857\n",
      "9       (0.153, 0.9]    141   0.322455    0.319149\n",
      "\n",
      "GLM calibration (head):\n",
      "                        bin  count  mean_pred  event_rate\n",
      "0  (-0.0009999503, 0.0187]    141   0.007602    0.000000\n",
      "1          (0.0187, 0.043]    141   0.030459    0.000000\n",
      "2          (0.043, 0.0742]    140   0.058527    0.000000\n",
      "3           (0.0742, 0.11]    141   0.091358    0.000000\n",
      "4            (0.11, 0.154]    140   0.130080    0.000000\n",
      "5           (0.154, 0.221]    141   0.187278    0.021277\n",
      "6           (0.221, 0.312]    140   0.262876    0.014286\n",
      "7           (0.312, 0.441]    141   0.367683    0.007092\n",
      "8           (0.441, 0.646]    140   0.534405    0.064286\n",
      "9           (0.646, 0.999]    141   0.835106    0.276596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/f43yv7vs72jc5wbt3yhjf7_m0000gn/T/ipykernel_8832/3242766122.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby('bin').agg(count=('y','size'), mean_pred=('p','mean'), event_rate=('y','mean'))\\\n",
      "/var/folders/2n/f43yv7vs72jc5wbt3yhjf7_m0000gn/T/ipykernel_8832/3242766122.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby('bin').agg(count=('y','size'), mean_pred=('p','mean'), event_rate=('y','mean'))\\\n",
      "/var/folders/2n/f43yv7vs72jc5wbt3yhjf7_m0000gn/T/ipykernel_8832/3242766122.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby('bin').agg(count=('y','size'), mean_pred=('p','mean'), event_rate=('y','mean'))\\\n"
     ]
    }
   ],
   "source": [
    "cal_tabs = {}\n",
    "for name, s in scores.items():\n",
    "    cal = calibration_table(y, s, n_bins=10)\n",
    "    cal_tabs[name] = cal\n",
    "    print(f\"\\n{name} calibration (head):\\n\", cal.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe39e9",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- If the top bin has mean_pred far above event_rate (e.g., 0.8 vs 0.28), the model is **over-confident** at the high end — typical with class weights.  \n",
    "- Plan: add a **post-hoc calibrator** (isotonic or Platt) later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde934b",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "* **Logit/GLM**: strong **overconfidence** (top bin mean_pred >> event rate) → probabilities are unreliable without calibration.\n",
    "* **RF**: **well calibrated** at the top bin; still verify after isotonic calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c413c",
   "metadata": {},
   "source": [
    "### Step 6 — Save summaries\n",
    "\n",
    "**Why:** Persist the threshold and calibration tables for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d31b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved threshold & calibration tables to /Users/reebal/FH-Wedel/WS25/seminar-bankruptcy-prediction/data/processed\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = DATA_DIR\n",
    "for name, tab in tables.items():\n",
    "    tab.to_csv(OUT_DIR / f'poland_h1_thresholds_{name.lower()}.csv', index=False)\n",
    "for name, tab in cal_tabs.items():\n",
    "    tab.to_csv(OUT_DIR / f'poland_h1_calibration_{name.lower()}.csv', index=False)\n",
    "print('Saved threshold & calibration tables to', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9ae80",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Confirm CSVs were written; we’ll cite them in the comparison notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28c77e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
