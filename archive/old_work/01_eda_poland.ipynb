{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b50e56",
   "metadata": {},
   "source": [
    "# 01 — EDA (Polish Companies Bankruptcy)\n",
    "\n",
    "**Goal:** sanity-check and audit the Polish bankruptcy datasets (1–5 year horizons) and prepare them for modeling.\n",
    "We will:\n",
    "1. Locate and verify the ARFF files (1year…5year)\n",
    "2. Load each file, attach a `horizon` column, harmonize columns\n",
    "3. Check shapes, dtypes, target encoding, missingness\n",
    "4. Summarize class balance by horizon\n",
    "\n",
    "> After each **code** cell you'll find a placeholder markdown cell titled **Interpretation**.  \n",
    "> Leave it empty for now — once you run a cell and see the output, paste your short interpretation there (or keep it as notes for us to fill together).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd616c3",
   "metadata": {},
   "source": [
    "### Step 1 — Import libraries\n",
    "\n",
    "**Why:** We set up all packages used for EDA and safe loading of ARFF data.\n",
    "If your environment is missing some packages, install them first (see notes in the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b2ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports OK. Pandas: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# If any imports fail, install the package in your env, e.g.:\n",
    "# uv pip install scipy statsmodels imbalanced-learn pyarrow\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import arff\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ Imports OK. Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c2b70",
   "metadata": {},
   "source": [
    "### Step 2 — Locate dataset files on disk\n",
    "\n",
    "**Why:** Ensure the expected ARFF files exist so subsequent steps don't fail.\n",
    "We keep paths **relative** to the repo root so notebooks are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44775b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/reebal/FH-Wedel/WS25/seminar-bankruptcy-prediction\n",
      "Data dir: /Users/reebal/FH-Wedel/WS25/seminar-bankruptcy-prediction/data/polish-companies-bankruptcy/polish+companies+bankruptcy+data\n",
      "Expected files: ['1year.arff', '2year.arff', '3year.arff', '4year.arff', '5year.arff']\n",
      "Found files: ['1year.arff', '2year.arff', '3year.arff', '4year.arff', '5year.arff']\n",
      "✅ All ARFF files present.\n"
     ]
    }
   ],
   "source": [
    "# Adjust REPO_ROOT if you open the notebook from another folder.\n",
    "REPO_ROOT = Path.cwd()\n",
    "DATA_DIR = REPO_ROOT / \"data\" / \"polish-companies-bankruptcy\" / \"polish+companies+bankruptcy+data\"\n",
    "\n",
    "expected_files = [DATA_DIR / f\"{i}year.arff\" for i in [1,2,3,4,5]]\n",
    "files_found = [p for p in expected_files if p.exists()]\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Data dir:\", DATA_DIR)\n",
    "print(\"Expected files:\", [p.name for p in expected_files])\n",
    "print(\"Found files:\", [p.name for p in files_found])\n",
    "\n",
    "assert len(files_found) == 5, \"❌ Not all ARFF files are present. Please check your path.\"\n",
    "print(\"✅ All ARFF files present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ad203",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- You should see 5 expected files and 5 found files listed.  \n",
    "- If the assertion fails, fix `REPO_ROOT` or the data directory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79533c4a",
   "metadata": {},
   "source": [
    "### Step 3 — Define robust ARFF → DataFrame loader\n",
    "\n",
    "**Why:** ARFF returns a structured array; we convert it into a clean `DataFrame` and standardize the target to integers `{0,1}`.\n",
    "We also attach the horizon (`1…5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831e8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arff_to_df(path: Path, horizon: int) -> pd.DataFrame:\n",
    "    data, meta = arff.loadarff(str(path))\n",
    "    df = pd.DataFrame(data)\n",
    "    # Decode byte columns that represent strings\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            try:\n",
    "                df[c] = df[c].str.decode('utf-8')\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Try to identify the target column (commonly 'class')\n",
    "    target_col_candidates = [c for c in df.columns if c.lower() in {'class', 'target', 'bankrupt'}]\n",
    "    if not target_col_candidates:\n",
    "        raise ValueError(f\"No obvious target column found in {path.name}. Columns: {list(df.columns)[:10]}...\")\n",
    "    target_col = target_col_candidates[0]\n",
    "\n",
    "    # Normalize target to {0,1}\n",
    "    y_raw = df[target_col]\n",
    "    # Common encodings in UCI sets: '0'/'1', 'N'/'B', 'No'/'Yes', 'negative'/'positive'\n",
    "    mapping = {\n",
    "        '0': 0, '1': 1,\n",
    "        'N': 0, 'B': 1,\n",
    "        'No': 0, 'Yes': 1,\n",
    "        'negative': 0, 'positive': 1,\n",
    "        'bankrupt': 1, 'non-bankrupt': 0,\n",
    "        'false': 0, 'true': 1,\n",
    "        0: 0, 1: 1\n",
    "    }\n",
    "    def map_target(v):\n",
    "        # convert bytes to str if needed\n",
    "        if isinstance(v, bytes):\n",
    "            v = v.decode('utf-8', errors='ignore')\n",
    "        return mapping.get(v, v)\n",
    "\n",
    "    df[target_col] = y_raw.map(map_target)\n",
    "    # If still not numeric 0/1, coerce cautiously\n",
    "    if not pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "        # try cast to int, else to category then to 0/1 by ordering\n",
    "        try:\n",
    "            df[target_col] = df[target_col].astype(int)\n",
    "        except Exception:\n",
    "            df[target_col] = pd.factorize(df[target_col])[0]\n",
    "            # ensure minority is coded as 1 if class is imbalanced\n",
    "            if df[target_col].value_counts().shape[0] == 2:\n",
    "                counts = df[target_col].value_counts()\n",
    "                minority = counts.idxmin()\n",
    "                df[target_col] = (df[target_col] == minority).astype(int)\n",
    "\n",
    "    # Attach horizon\n",
    "    df['horizon'] = horizon\n",
    "    # Standardize target name to 'y'\n",
    "    if target_col != 'y':\n",
    "        df = df.rename(columns={target_col: 'y'})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c4c75",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- No output expected. This defines a helper to load ARFF files and normalize the target to `y ∈ {0,1}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41606d70",
   "metadata": {},
   "source": [
    "### Step 4 — Load all horizons, check shapes and class balance\n",
    "\n",
    "**Why:** We need to confirm column consistency across horizons, dataset sizes, and the proportion of bankrupt firms per horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fb6045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes per horizon:\n",
      "  h=1: (7027, 66), bankrupt share (y=1) ≈ 0.039\n",
      "  h=2: (10173, 66), bankrupt share (y=1) ≈ 0.039\n",
      "  h=3: (10503, 66), bankrupt share (y=1) ≈ 0.047\n",
      "  h=4: (9792, 66), bankrupt share (y=1) ≈ 0.053\n",
      "  h=5: (5910, 66), bankrupt share (y=1) ≈ 0.069\n",
      "\n",
      "Columns consistent across horizons: True\n",
      "\n",
      "Combined shape: (43405, 66)\n",
      "Targets:  {0: 41314, 1: 2091}\n",
      "Horizon counts: {1: 7027, 2: 10173, 3: 10503, 4: 9792, 5: 5910}\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for h in [1,2,3,4,5]:\n",
    "    df_h = load_arff_to_df(DATA_DIR / f\"{h}year.arff\", horizon=h)\n",
    "    dfs.append(df_h)\n",
    "\n",
    "# Column consistency\n",
    "cols_per_h = {h: set(df_h.columns) for h, df_h in zip([1,2,3,4,5], dfs)}\n",
    "all_equal = len({tuple(sorted(cols)) for cols in cols_per_h.values()}) == 1\n",
    "\n",
    "print(\"Shapes per horizon:\")\n",
    "for h, df_h in zip([1,2,3,4,5], dfs):\n",
    "    pos_rate = df_h['y'].mean() if 'y' in df_h.columns else np.nan\n",
    "    print(f\"  h={h}: {df_h.shape}, bankrupt share (y=1) ≈ {pos_rate:.3f}\")\n",
    "\n",
    "print(\"\\nColumns consistent across horizons:\", all_equal)\n",
    "if not all_equal:\n",
    "    # print a small diff\n",
    "    base_cols = cols_per_h[1]\n",
    "    for h in [2,3,4,5]:\n",
    "        diff_add = sorted(list(cols_per_h[h] - base_cols))\n",
    "        diff_drop = sorted(list(base_cols - cols_per_h[h]))\n",
    "        if diff_add or diff_drop:\n",
    "            print(f\"  vs h=1 → h={h}: +{len(diff_add)} cols, -{len(diff_drop)} cols\")\n",
    "            if diff_add: print(\"    added:\", diff_add[:10], (\"…\" if len(diff_add)>10 else \"\"))\n",
    "            if diff_drop: print(\"    dropped:\", diff_drop[:10], (\"…\" if len(diff_drop)>10 else \"\"))\n",
    "\n",
    "df_all = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "print(\"\\nCombined shape:\", df_all.shape)\n",
    "print(\"Targets: \", df_all['y'].value_counts(dropna=False).to_dict())\n",
    "print(\"Horizon counts:\", df_all['horizon'].value_counts().sort_index().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53dfada",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Note dataset sizes by horizon and the bankrupt share; this gives you an early view of class imbalance.  \n",
    "- If columns are not consistent, we’ll need to align them (e.g., intersect features).  \n",
    "- Confirm that `y` has only `{0,1}` and that the overall positive rate is plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4bb0b",
   "metadata": {},
   "source": [
    "## Step 4 — Interpretation\n",
    "\n",
    "* **Consistent schema:** All horizons share the same 64 features → great, we can concatenate safely and compare horizons directly without extra alignment work.\n",
    "* **Class imbalance:** Positives (bankrupt) rise from **3.9% (h=1)** to **6.9% (h=5)**, which is exactly what we expect: a longer “look-ahead” window captures more future bankruptcies. Overall rate ≈ **4.8%** (2,091 of 43,405). That’s imbalanced but absolutely usable with class-weighted learners and PR-AUC–oriented evaluation.\n",
    "* **Sample size:** Each horizon has thousands of firms (e.g., 7k–10k rows for h=1–4). That’s enough for robust CV and for holding out a test fold.\n",
    "* **Early-warning framing:** For a strict *Frühwarnsystem*, the 2–3 year horizons are arguably the most interesting; we’ll still start modeling with **h=1** as a baseline and then compare h=2–3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3e714",
   "metadata": {},
   "source": [
    "### Step 5 — Dtypes and missingness snapshot\n",
    "\n",
    "**Why:** We need to understand variable types and the extent of missing data before modeling. This informs cleaning and winsorization plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48aa0fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtypes (first 15):\n",
      "Attr1     float64\n",
      "Attr2     float64\n",
      "Attr3     float64\n",
      "Attr4     float64\n",
      "Attr5     float64\n",
      "Attr6     float64\n",
      "Attr7     float64\n",
      "Attr8     float64\n",
      "Attr9     float64\n",
      "Attr10    float64\n",
      "Attr11    float64\n",
      "Attr12    float64\n",
      "Attr13    float64\n",
      "Attr14    float64\n",
      "Attr15    float64\n",
      "dtype: object\n",
      "\n",
      "Missingness (top 20):\n",
      "Attr37    0.437369\n",
      "Attr21    0.134869\n",
      "Attr27    0.063679\n",
      "Attr60    0.049580\n",
      "Attr45    0.049464\n",
      "Attr24    0.021242\n",
      "Attr64    0.018708\n",
      "Attr53    0.018708\n",
      "Attr28    0.018708\n",
      "Attr54    0.018708\n",
      "Attr41    0.017371\n",
      "Attr32    0.008478\n",
      "Attr52    0.006935\n",
      "Attr47    0.006843\n",
      "Attr46    0.003110\n",
      "Attr4     0.003087\n",
      "Attr33    0.003087\n",
      "Attr40    0.003087\n",
      "Attr12    0.003087\n",
      "Attr63    0.003087\n",
      "dtype: float64\n",
      "\n",
      "Class balance by horizon:\n",
      "         count  pos_rate\n",
      "horizon                 \n",
      "1         7027  0.038566\n",
      "2        10173  0.039320\n",
      "3        10503  0.047129\n",
      "4         9792  0.052594\n",
      "5         5910  0.069374\n"
     ]
    }
   ],
   "source": [
    "print(\"dtypes (first 15):\") \n",
    "print(df_all.dtypes.head(15))\n",
    "\n",
    "# Basic missingness\n",
    "na_pct = df_all.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissingness (top 20):\") \n",
    "print(na_pct.head(20))\n",
    "\n",
    "# Quick look at y, horizon joint distribution\n",
    "print(\"\\nClass balance by horizon:\") \n",
    "print(df_all.groupby('horizon')['y'].agg(['count','mean']).rename(columns={'mean':'pos_rate'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e1900",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- List any non-numeric columns that look like financial ratios (they should be numeric).  \n",
    "- Identify variables with heavy missingness (e.g., >30%) to consider for dropping or imputation.  \n",
    "- Comment on how the bankrupt share changes with the horizon (expected: nearest horizon has higher event rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78278043",
   "metadata": {},
   "source": [
    "## Step 5 — Interpretation\n",
    "\n",
    "* **Types:** All features are numeric (`float64`) → convenient for modeling and statistics.\n",
    "* **Missingness:** One feature, **Attr37**, shows **~44% NA** (very high). A second, **Attr21**, sits at ~13%. The rest are mostly below 6%.\n",
    "\n",
    "  * Keeping a 44%–missing ratio and blindly imputing can inject noise; often **dropping** such a feature is cleaner (unless metadata says it’s essential).\n",
    "  * For features with **≥5%** NA, it’s useful to add a **missingness indicator**; sometimes “missing” is informative in financial statements.\n",
    "* **Class balance by horizon:** Positive rate increases monotonically from h=1 → h=5, as noted above. Good sanity check.\n",
    "\n",
    "**Limitations to keep in mind:**\n",
    "\n",
    "* The dataset doesn’t include firm IDs or calendar years (just `horizon` and ratios). That means we **cannot** do a strict time-based split or clustered CV by firm. We’ll be transparent about this limitation and (a) evaluate models **within a horizon** via stratified CV and (b) assess **cross-horizon robustness** (e.g., train on h=1, test on h=2/3) as a proxy for stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
