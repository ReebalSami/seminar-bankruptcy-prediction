{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e302deb",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Baseline Models (Poland, Horizon = 1)\n",
    "\n",
    "**Goal:** Establish clean baselines on the Polish *1-year* horizon:\n",
    "- **Logistic Regression** (sklearn, class-weighted) for a strong linear baseline\n",
    "- **Random Forest** (class-weighted) for a non-linear baseline robust to collinearity\n",
    "- **GLM Binomial (logit)** with robust SEs (statsmodels) for interpretable coefficients\n",
    "\n",
    "**Metrics to report:**\n",
    "- PR-AUC (average precision), ROC-AUC, **Brier score** (calibration)\n",
    "- **Recall at FPR ‚â§ 1%** and **‚â§ 5%** (early-warning operating points)\n",
    "- Reliability (binned calibration) table\n",
    "\n",
    "> We will use `poland_clean_full.parquet` for RF and `poland_clean_reduced.parquet` for Logit/GLM (to stabilize inference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764bc5e",
   "metadata": {},
   "source": [
    "### Step 1 ‚Äî Imports\n",
    "\n",
    "**Why:** Bring in modeling & metrics utilities. If imports fail, install locally:\n",
    "```\n",
    "uv pip install scikit-learn statsmodels pyarrow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ca1293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports OK\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, roc_curve, precision_recall_curve\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print('‚úÖ Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621888d7",
   "metadata": {},
   "source": [
    "*No interpretation needed here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a75b8",
   "metadata": {},
   "source": [
    "### Step 2 ‚Äî Load cleaned datasets and subset to horizon = 1\n",
    "\n",
    "**Why:** We‚Äôll train/evaluate on the 1-year horizon first.  \n",
    "- `full`: winsorized+imputed with indicators (for RF)  \n",
    "- `reduced`: correlation-pruned set (for logistic inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94211776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_h1: (7027, 65) | pos rate = 0.0386\n",
      "red_h1 : (7027, 48) | pos rate = 0.0386\n"
     ]
    }
   ],
   "source": [
    "REPO_ROOT = Path.cwd()\n",
    "DATA_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "df_full = pd.read_parquet(DATA_DIR / \"poland_clean_full.parquet\")\n",
    "df_red  = pd.read_parquet(DATA_DIR / \"poland_clean_reduced.parquet\")\n",
    "\n",
    "# Filter to horizon = 1\n",
    "full_h1 = df_full[df_full['horizon'] == 1].copy()\n",
    "red_h1  = df_red[df_red['horizon'] == 1].copy()\n",
    "\n",
    "y_full = full_h1['y'].astype(int).values\n",
    "X_full = full_h1.drop(columns=['y', 'horizon'])\n",
    "y_red  = red_h1['y'].astype(int).values\n",
    "X_red  = red_h1.drop(columns=['y', 'horizon'])\n",
    "\n",
    "print('full_h1:', X_full.shape, '| pos rate =', y_full.mean().round(4))\n",
    "print('red_h1 :', X_red.shape,  '| pos rate =', y_red.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e02b7",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Confirm sample size (~7k rows) and ~3.9% positives for both views.  \n",
    "- `X_red` should have fewer features than `X_full` (by design)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcdebd5",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äî Stratified train/test split (80/20)\n",
    "\n",
    "**Why:** Hold out a test fold for honest evaluation. We use stratification to preserve the rare-event rate in both folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38b03e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5621 | Test size: 1406\n",
      "Train pos rate: 0.0386 | Test pos rate: 0.0384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "(train_idx, test_idx), = sss.split(X_full, y_full)\n",
    "\n",
    "Xf_tr, Xf_te = X_full.iloc[train_idx], X_full.iloc[test_idx]\n",
    "yf_tr, yf_te = y_full[train_idx], y_full[test_idx]\n",
    "\n",
    "Xr_tr, Xr_te = X_red.iloc[train_idx], X_red.iloc[test_idx]  # match same indices for comparability\n",
    "yr_tr, yr_te = y_red[train_idx], y_red[test_idx]\n",
    "\n",
    "print('Train size:', Xf_tr.shape[0], '| Test size:', Xf_te.shape[0])\n",
    "print('Train pos rate:', yf_tr.mean().round(4), '| Test pos rate:', yf_te.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9cf4e2",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Train/test sizes should be close to 80/20.  \n",
    "- Positive rates should be similar across folds (¬±0.2pp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2a022",
   "metadata": {},
   "source": [
    "### Step 4 ‚Äî Helper functions for evaluation\n",
    "\n",
    "**Why:** Consistent metrics + early-warning recall at fixed FPR caps; calibration table; nice one-liner summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dc1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, roc_curve\n",
    "\n",
    "def recall_at_fpr(y_true, y_score, fpr_cap=0.01):\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_score)\n",
    "    mask = fpr <= fpr_cap\n",
    "    if not np.any(mask):\n",
    "        return 0.0, None\n",
    "    idx = np.argmax(tpr[mask])  # best recall within cap\n",
    "    return float(tpr[mask][idx]), float(thr[mask][idx])\n",
    "\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    bins = pd.qcut(y_prob, q=n_bins, duplicates='drop')\n",
    "    tab = pd.DataFrame({'bin': bins, 'y': y_true, 'p': y_prob})\\\n",
    "        .groupby('bin').agg(count=('y','size'), mean_pred=('p','mean'), event_rate=('y','mean'))\\\n",
    "        .reset_index()\n",
    "    return tab\n",
    "\n",
    "def evaluate_scores(y_true, y_prob):\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    pr  = average_precision_score(y_true, y_prob)\n",
    "    b   = brier_score_loss(y_true, y_prob)\n",
    "    rec1, thr1 = recall_at_fpr(y_true, y_prob, 0.01)\n",
    "    rec5, thr5 = recall_at_fpr(y_true, y_prob, 0.05)\n",
    "    return {'roc_auc': roc, 'pr_auc': pr, 'brier': b, 'rec1': rec1, 'thr1': thr1, 'rec5': rec5, 'thr5': thr5}\n",
    "\n",
    "def print_eval(label, res):\n",
    "    print(f\"[{label}] ROC-AUC={res['roc_auc']:.3f} | PR-AUC={res['pr_auc']:.3f} | Brier={res['brier']:.4f} | \"\n",
    "          f\"Recall@1%FPR={res['rec1']:.3f} (thr={res['thr1']}) | Recall@5%FPR={res['rec5']:.3f} (thr={res['thr5']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186a29",
   "metadata": {},
   "source": [
    "*No interpretation needed here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f1139",
   "metadata": {},
   "source": [
    "### Step 5 ‚Äî Logistic Regression (sklearn, class-weighted)\n",
    "\n",
    "**Why:** Strong linear baseline; we scale features and tune `C` on the training fold via 5-fold CV (scoring = PR-AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7e4683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: {'clf__C': np.float64(1.0)}\n",
      "[Logit (sklearn)] ROC-AUC=0.924 | PR-AUC=0.385 | Brier=0.1064 | Recall@1%FPR=0.278 (thr=0.9817998903019366) | Recall@5%FPR=0.648 (thr=0.7449873031547386)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>count</th>\n",
       "      <th>mean_pred</th>\n",
       "      <th>event_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.000999895, 0.0195]</td>\n",
       "      <td>141</td>\n",
       "      <td>0.008294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0195, 0.045]</td>\n",
       "      <td>141</td>\n",
       "      <td>0.031962</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.045, 0.0767]</td>\n",
       "      <td>140</td>\n",
       "      <td>0.060557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0767, 0.112]</td>\n",
       "      <td>141</td>\n",
       "      <td>0.093760</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.112, 0.156]</td>\n",
       "      <td>140</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.156, 0.223]</td>\n",
       "      <td>141</td>\n",
       "      <td>0.189368</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.223, 0.311]</td>\n",
       "      <td>140</td>\n",
       "      <td>0.265087</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.311, 0.441]</td>\n",
       "      <td>141</td>\n",
       "      <td>0.369789</td>\n",
       "      <td>0.014184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.441, 0.65]</td>\n",
       "      <td>140</td>\n",
       "      <td>0.533753</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.65, 0.999]</td>\n",
       "      <td>141</td>\n",
       "      <td>0.834228</td>\n",
       "      <td>0.276596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bin  count  mean_pred  event_rate\n",
       "0  (-0.000999895, 0.0195]    141   0.008294    0.000000\n",
       "1         (0.0195, 0.045]    141   0.031962    0.000000\n",
       "2         (0.045, 0.0767]    140   0.060557    0.000000\n",
       "3         (0.0767, 0.112]    141   0.093760    0.000000\n",
       "4          (0.112, 0.156]    140   0.132571    0.000000\n",
       "5          (0.156, 0.223]    141   0.189368    0.021277\n",
       "6          (0.223, 0.311]    140   0.265087    0.014286\n",
       "7          (0.311, 0.441]    141   0.369789    0.014184\n",
       "8           (0.441, 0.65]    140   0.533753    0.057143\n",
       "9           (0.65, 0.999]    141   0.834228    0.276596"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logit_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', LogisticRegression(\n",
    "        penalty='l2', solver='liblinear',\n",
    "        class_weight='balanced', max_iter=200, random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {'clf__C': np.logspace(-2, 2, 9)}  # 0.01 ... 100\n",
    "gs_logit = GridSearchCV(logit_pipe, param_grid=param_grid, scoring='average_precision',\n",
    "                        cv=5, n_jobs=-1, refit=True, verbose=0)\n",
    "gs_logit.fit(Xr_tr, yr_tr)\n",
    "\n",
    "print('Best C:', gs_logit.best_params_)\n",
    "proba_logit = gs_logit.predict_proba(Xr_te)[:,1]\n",
    "res_logit = evaluate_scores(yr_te, proba_logit)\n",
    "print_eval('Logit (sklearn)', res_logit)\n",
    "\n",
    "# Calibration table (test fold)\n",
    "cal_logit = calibration_table(yr_te, proba_logit, n_bins=10)\n",
    "cal_logit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53df31",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Note the chosen `C` and whether performance is balanced (ROC-AUC vs PR-AUC).  \n",
    "- Focus on **Recall@1% FPR** ‚Äî if it‚Äôs very low, we may need stronger regularization or more features.  \n",
    "- Review the calibration table: mean predicted ‚âà event rate in bins ‚Üí good calibration; big gaps ‚Üí consider calibration later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efae51",
   "metadata": {},
   "source": [
    "### Step 6 ‚Äî Random Forest (class-weighted)\n",
    "\n",
    "**Why:** Non-linear, robust to collinearity/outliers; tune shallow-to-moderate depth to avoid overfitting rare events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c456254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': None, 'min_samples_leaf': 5}\n",
      "[RandomForest] ROC-AUC=0.968 | PR-AUC=0.738 | Brier=0.0208 | Recall@1%FPR=0.593 (thr=0.311743306538647) | Recall@5%FPR=0.796 (thr=0.18709312710506992)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attr27__isna    0.099219\n",
       "Attr27          0.066737\n",
       "Attr24          0.041010\n",
       "Attr13          0.034497\n",
       "Attr34          0.034312\n",
       "Attr26          0.030567\n",
       "Attr46          0.027151\n",
       "Attr9           0.021988\n",
       "Attr6           0.021905\n",
       "Attr16          0.021877\n",
       "Attr11          0.019498\n",
       "Attr58          0.017776\n",
       "Attr21          0.017438\n",
       "Attr5           0.016978\n",
       "Attr19          0.016508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400, random_state=RANDOM_STATE,\n",
    "    class_weight='balanced_subsample', n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 4, 6, 8, 12],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "gs_rf = GridSearchCV(rf, param_grid=param_grid,\n",
    "                     cv=5, scoring='average_precision', n_jobs=-1, refit=True, verbose=0)\n",
    "gs_rf.fit(Xf_tr, yf_tr)\n",
    "\n",
    "print('Best params:', gs_rf.best_params_)\n",
    "proba_rf = gs_rf.predict_proba(Xf_te)[:,1]\n",
    "res_rf = evaluate_scores(yf_te, proba_rf)\n",
    "print_eval('RandomForest', res_rf)\n",
    "\n",
    "# Feature importances (top 15)\n",
    "imp = pd.Series(gs_rf.best_estimator_.feature_importances_, index=Xf_tr.columns).sort_values(ascending=False).head(15)\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19cee3",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Compare PR-AUC and Recall@1%FPR to the Logit baseline.  \n",
    "- Inspect top importances ‚Äî do they cluster by profitability, leverage, liquidity, etc.? This helps connect to theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bc80a",
   "metadata": {},
   "source": [
    "### Step 7 ‚Äî GLM Binomial (logit) with robust SEs (statsmodels)\n",
    "\n",
    "**Why:** For interpretable coefficients with **robust (HC1) standard errors**.  \n",
    "We use the **reduced** feature set (collinearity-pruned) and class weights via `freq_weights` to approximate balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d636856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLM Binomial (robust)] ROC-AUC=0.924 | PR-AUC=0.395 | Brier=0.1061 | Recall@1%FPR=0.278 (thr=0.9828315289483237) | Recall@5%FPR=0.667 (thr=0.7333376160367324)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>std_err</th>\n",
       "      <th>z</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>-1.703393</td>\n",
       "      <td>0.043059</td>\n",
       "      <td>-39.559786</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Attr27__isna</td>\n",
       "      <td>1.089711</td>\n",
       "      <td>0.039069</td>\n",
       "      <td>27.891917</td>\n",
       "      <td>3.343837e-171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Attr21__isna</td>\n",
       "      <td>0.487463</td>\n",
       "      <td>0.031053</td>\n",
       "      <td>15.697618</td>\n",
       "      <td>1.570277e-55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Attr41</td>\n",
       "      <td>-0.478438</td>\n",
       "      <td>0.037644</td>\n",
       "      <td>-12.709502</td>\n",
       "      <td>5.236679e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Attr34</td>\n",
       "      <td>1.090835</td>\n",
       "      <td>0.092647</td>\n",
       "      <td>11.774044</td>\n",
       "      <td>5.311593e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attr15</td>\n",
       "      <td>0.258086</td>\n",
       "      <td>0.022772</td>\n",
       "      <td>11.333277</td>\n",
       "      <td>8.978184e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Attr39</td>\n",
       "      <td>-1.573863</td>\n",
       "      <td>0.181220</td>\n",
       "      <td>-8.684798</td>\n",
       "      <td>3.794174e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Attr21</td>\n",
       "      <td>-0.294944</td>\n",
       "      <td>0.037155</td>\n",
       "      <td>-7.938227</td>\n",
       "      <td>2.050910e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Attr38</td>\n",
       "      <td>-0.571141</td>\n",
       "      <td>0.077020</td>\n",
       "      <td>-7.415515</td>\n",
       "      <td>1.211529e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Attr40</td>\n",
       "      <td>0.685965</td>\n",
       "      <td>0.105926</td>\n",
       "      <td>6.475868</td>\n",
       "      <td>9.426839e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Attr25</td>\n",
       "      <td>-0.364018</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>-6.263414</td>\n",
       "      <td>3.766381e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Attr50</td>\n",
       "      <td>-0.837437</td>\n",
       "      <td>0.140908</td>\n",
       "      <td>-5.943159</td>\n",
       "      <td>2.795810e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Attr30</td>\n",
       "      <td>0.411898</td>\n",
       "      <td>0.070729</td>\n",
       "      <td>5.823595</td>\n",
       "      <td>5.759522e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Attr35</td>\n",
       "      <td>0.850666</td>\n",
       "      <td>0.156047</td>\n",
       "      <td>5.451352</td>\n",
       "      <td>4.998823e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Attr33</td>\n",
       "      <td>-0.452600</td>\n",
       "      <td>0.085811</td>\n",
       "      <td>-5.274369</td>\n",
       "      <td>1.332136e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature      coef   std_err          z           pval\n",
       "0          const -1.703393  0.043059 -39.559786   0.000000e+00\n",
       "48  Attr27__isna  1.089711  0.039069  27.891917  3.343837e-171\n",
       "47  Attr21__isna  0.487463  0.031053  15.697618   1.570277e-55\n",
       "25        Attr41 -0.478438  0.037644 -12.709502   5.236679e-37\n",
       "19        Attr34  1.090835  0.092647  11.774044   5.311593e-32\n",
       "4         Attr15  0.258086  0.022772  11.333277   8.978184e-30\n",
       "23        Attr39 -1.573863  0.181220  -8.684798   3.794174e-18\n",
       "8         Attr21 -0.294944  0.037155  -7.938227   2.050910e-15\n",
       "22        Attr38 -0.571141  0.077020  -7.415515   1.211529e-13\n",
       "24        Attr40  0.685965  0.105926   6.475868   9.426839e-11\n",
       "10        Attr25 -0.364018  0.058118  -6.263414   3.766381e-10\n",
       "34        Attr50 -0.837437  0.140908  -5.943159   2.795810e-09\n",
       "15        Attr30  0.411898  0.070729   5.823595   5.759522e-09\n",
       "20        Attr35  0.850666  0.156047   5.451352   4.998823e-08\n",
       "18        Attr33 -0.452600  0.085811  -5.274369   1.332136e-07"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xr_tr_scaled = scaler.fit_transform(Xr_tr)\n",
    "Xr_te_scaled = scaler.transform(Xr_te)\n",
    "\n",
    "Xsm_tr = sm.add_constant(Xr_tr_scaled, has_constant='add')\n",
    "Xsm_te = sm.add_constant(Xr_te_scaled, has_constant='add')\n",
    "\n",
    "# class-balance weights\n",
    "pos_w = (len(yr_tr) - yr_tr.sum()) / (yr_tr.sum())\n",
    "weights = np.where(yr_tr==1, pos_w, 1.0)\n",
    "\n",
    "glm_binom = sm.GLM(yr_tr, Xsm_tr, family=sm.families.Binomial(), freq_weights=weights)\n",
    "glm_res = glm_binom.fit(cov_type='HC1')\n",
    "\n",
    "proba_glm = glm_res.predict(Xsm_te)\n",
    "res_glm = evaluate_scores(yr_te, proba_glm)\n",
    "print_eval('GLM Binomial (robust)', res_glm)\n",
    "\n",
    "coefs = pd.DataFrame({\n",
    "    'feature': ['const'] + list(Xr_tr.columns),\n",
    "    'coef': glm_res.params,\n",
    "    'std_err': glm_res.bse,\n",
    "    'z': glm_res.tvalues,\n",
    "    'pval': glm_res.pvalues\n",
    "}).sort_values('z', key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "coefs.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9943e",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Compare GLM performance to sklearn Logit ‚Äî they should be close.  \n",
    "- Check sign/magnitude against bankruptcy theory (profitability ‚Üì risk, leverage/liquidity ‚Üë risk, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13822932",
   "metadata": {},
   "source": [
    "### Step 8 ‚Äî Save test-set predictions for later analysis\n",
    "\n",
    "**Why:** We‚Äôll use these to plot decision curves / make threshold analyses in the comparison notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4b43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/reebal/FH-Wedel/WS25/seminar-bankruptcy-prediction/data/processed/poland_h1_test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = DATA_DIR\n",
    "preds = pd.DataFrame({\n",
    "    'y_true': yf_te,\n",
    "    'p_logit': proba_logit,\n",
    "    'p_rf': proba_rf,\n",
    "    'p_glm': proba_glm\n",
    "})\n",
    "out_path = OUT_DIR / 'poland_h1_test_predictions.csv'\n",
    "preds.to_csv(out_path, index=False)\n",
    "print('Saved:', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5780c8",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Confirm the CSV was saved to `data/processed/poland_h1_test_predictions.csv`.  \n",
    "- We‚Äôll reuse it when comparing models/datasets next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dfdbe",
   "metadata": {},
   "source": [
    "üî• Solid first baselines. Here‚Äôs a straight, critical read of what you got, plus exactly what to do next.\n",
    "\n",
    "# What the results say\n",
    "\n",
    "## Logistic Regression (sklearn)\n",
    "\n",
    "* **ROC-AUC 0.924, PR-AUC 0.385** ‚Üí good ranking overall, but not great in the rare-event region compared to RF.\n",
    "* **Recall@1% FPR = 0.278** (threshold ‚âà 0.982) ‚Üí low catch rate under strict false-alarm budgets.\n",
    "* **Calibration is poor**: In the top bin, **mean_pred ‚âà 0.834 vs event_rate ‚âà 0.277**. That‚Äôs massive **overconfidence**, typical when using `class_weight='balanced'` for rare events ‚Äî it helps ranking, hurts probability calibration.\n",
    "* **Brier 0.106** is relatively high (but remember: with very low base rates, Brier is dominated by negatives).\n",
    "\n",
    "üëâ Conclusion: fine as an interpretable baseline, but **not** the best early-warning detector under tight FPR caps without calibration.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "* **ROC-AUC 0.968, PR-AUC 0.738** ‚Üí **excellent**; huge uplift vs. Logit in the rare-event regime.\n",
    "* **Recall@1% FPR = 0.593**, **@5% FPR = 0.796** ‚Üí much better early-warning capture at the same false-alarm budgets.\n",
    "* **Top features** include `Attr27__isna` (missingness indicator) and `Attr27` itself, plus several ratios (e.g., `Attr24`, `Attr13`, `Attr34`, `Attr26`).\n",
    "\n",
    "  * Missingness being #1 says: **lack of certain disclosures is itself predictive** ‚Äî plausible in financial distress. This is valid because ‚Äúmissing vs. present‚Äù is known at prediction time. Still, we‚Äôll do a robustness check by training **without** indicators to make sure performance doesn‚Äôt collapse (guarding against accidental leakage).\n",
    "* **Brier 0.0208** is very low. With rare events, that often reflects conservative probabilities; we still need a **calibration check** (Brier alone can be misleading.\n",
    "\n",
    "üëâ Conclusion: **RF is your best detector** right now. Keep it as the primary scorer.\n",
    "\n",
    "## GLM Binomial (robust SEs)\n",
    "\n",
    "* **ROC-AUC 0.924, PR-AUC 0.395** ‚Üí similar to sklearn Logit (as expected).\n",
    "* **Recall@1% FPR = 0.278**; again, weak in the strict low-FPR zone without calibration.\n",
    "* **Inference looks sensible** (signs & large |z| for some features), but we‚Äôll only narrate economics once we map `Attr*` ‚Üí ratio names (see ‚ÄúNext steps‚Äù below).\n",
    "\n",
    "  * E.g., positive coefficients (‚Üë risk): `Attr34`, `Attr15`, `Attr40`, `Attr35`\n",
    "  * Negative (‚Üì risk): `Attr41`, `Attr39`, `Attr38`, `Attr25`, `Attr50`\n",
    "\n",
    "üëâ Conclusion: Use GLM for **interpretation** (with robust SEs) and RF for **detection**.\n",
    "\n",
    "---\n",
    "\n",
    "# What this means in practical alert terms (approx.)\n",
    "\n",
    "Assuming your test fold is ~20% of 7,027 (‚âà1,405 obs, ~3.9% bankrupt ‚âà **55 positives**, **1,350 negatives**):\n",
    "\n",
    "* **RF @ 1% FPR** ‚Üí recall 0.593\n",
    "\n",
    "  * **TP ‚âà 33**, **FP ‚âà 13**, **FN ‚âà 22**\n",
    "  * That‚Äôs ~**46** alerts total (**~33** are real), i.e., **~33 true bankruptcies caught** with only ~13 false alarms.\n",
    "* **Logit/GLM @ 1% FPR** ‚Üí recall ~0.278\n",
    "\n",
    "  * **TP ‚âà 15**, **FP ‚âà 13** ‚Äî materially fewer true catches at same false-alarm budget.\n",
    "\n",
    "These back-of-envelope numbers are helpful for stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "# Immediate fixes / upgrades\n",
    "\n",
    "1. **Calibrate probabilities** (especially for Logit/GLM; also check RF):\n",
    "\n",
    "   * Use `CalibratedClassifierCV` with **isotonic** (or Platt/sigmoid) on a validation split inside CV.\n",
    "   * Re-check Brier + reliability curves. Expect Logit calibration to improve a lot.\n",
    "\n",
    "2. **Choose operational thresholds** at **1% and 5% FPR**: quantify expected TP/FP and **alerts per 1,000 firms** ‚Äî easy to communicate.\n",
    "\n",
    "3. **Robustness checks**:\n",
    "\n",
    "   * Refit RF **without missingness indicators** (`__isna` cols). If performance barely moves, great; if it tanks, we‚Äôll document that ‚Äúmissingness is key‚Äù but still valid.\n",
    "   * Try **Logit (L1)** for sparser coefficients (cleaner interpretation). Keep Logit/GLM in the thesis even if RF wins on detection.\n",
    "\n",
    "4. **Map `Attr*` ‚Üí real ratio names** from the dataset docs/metadata file so your GLM table speaks ‚Äúfinance‚Äù (profitability, leverage, liquidity, activity).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
