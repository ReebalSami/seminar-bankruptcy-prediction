{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5c7fe5",
   "metadata": {},
   "source": [
    "# 01 — EDA (Polish Companies Bankruptcy) — Part 2\n",
    "\n",
    "**Goal (Steps 6–9):** Turn the audit into concrete cleaning decisions and diagnostics:\n",
    "- Decide on missingness handling (drop high-NA features; impute others; add missing indicators when useful)\n",
    "- Winsorize outliers per horizon (robustify heavy-tailed ratios)\n",
    "- Diagnose multicollinearity with correlations & VIF\n",
    "- Prepare a *candidate* cleaned feature set and save artifacts\n",
    "\n",
    "> Run this notebook from your repo root so the relative `data/` paths resolve.\n",
    "> If you prefer to continue inside your Part 1 notebook, you can copy-paste the cells instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4c3cd",
   "metadata": {},
   "source": [
    "### Step 6 — Imports & constants\n",
    "\n",
    "**Why:** Load required packages and set deterministic options. We'll reuse the ARFF loader from Part 1 to re-create `df_all` locally here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa77ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports OK\n"
     ]
    }
   ],
   "source": [
    "# If any imports fail, install them in your environment:\n",
    "# uv pip install scipy statsmodels pyarrow\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from collections import defaultdict\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "DATA_DIR = REPO_ROOT / \"data\" / \"polish-companies-bankruptcy\" / \"polish+companies+bankruptcy+data\"\n",
    "OUT_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('✅ Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d75089",
   "metadata": {},
   "source": [
    "*(No interpretation needed here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3812bfb",
   "metadata": {},
   "source": [
    "### Step 7 — Reload all horizons into a single DataFrame\n",
    "\n",
    "**Why:** Keep this notebook self-contained. We recreate `df_all` with a `y` target and `horizon` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882b3fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all shape: (43405, 66) | features: 64\n"
     ]
    }
   ],
   "source": [
    "def load_arff_to_df(path: Path, horizon: int) -> pd.DataFrame:\n",
    "    data, meta = arff.loadarff(str(path))\n",
    "    df = pd.DataFrame(data)\n",
    "    # Decode byte columns that represent strings\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            try:\n",
    "                df[c] = df[c].str.decode('utf-8')\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Identify target column\n",
    "    target_col_candidates = [c for c in df.columns if c.lower() in {'class', 'target', 'bankrupt'}]\n",
    "    if not target_col_candidates:\n",
    "        raise ValueError(f\"No obvious target column found in {path.name}. Columns: {list(df.columns)[:10]}...\")\n",
    "    target_col = target_col_candidates[0]\n",
    "\n",
    "    # Normalize target to {0,1}\n",
    "    mapping = {'0':0,'1':1,'N':0,'B':1,'No':0,'Yes':1,'negative':0,'positive':1,'bankrupt':1,'non-bankrupt':0,'false':0,'true':1,0:0,1:1}\n",
    "    def map_target(v):\n",
    "        if isinstance(v, bytes):\n",
    "            v = v.decode('utf-8', errors='ignore')\n",
    "        return mapping.get(v, v)\n",
    "    df[target_col] = df[target_col].map(map_target)\n",
    "    if not pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "        try:\n",
    "            df[target_col] = df[target_col].astype(int)\n",
    "        except Exception:\n",
    "            df[target_col] = pd.factorize(df[target_col])[0]\n",
    "            if df[target_col].value_counts().shape[0] == 2:\n",
    "                counts = df[target_col].value_counts()\n",
    "                minority = counts.idxmin()\n",
    "                df[target_col] = (df[target_col] == minority).astype(int)\n",
    "\n",
    "    df['horizon'] = horizon\n",
    "    if target_col != 'y':\n",
    "        df = df.rename(columns={target_col: 'y'})\n",
    "    return df\n",
    "\n",
    "dfs = [load_arff_to_df(DATA_DIR / f\"{h}year.arff\", h) for h in [1,2,3,4,5]]\n",
    "df_all = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "feature_cols = [c for c in df_all.columns if c not in ['y', 'horizon']]\n",
    "print('df_all shape:', df_all.shape, '| features:', len(feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f446f3",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Confirm the shape and that you have 64 feature columns plus `y` and `horizon` (total 66)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b5d5a",
   "metadata": {},
   "source": [
    "### Step 8 — Missingness decisions and imputation plan\n",
    "\n",
    "**Why:** Attr37 has ~44% missing; a few others are 5–13%. We will:\n",
    "1. **Drop** features with missingness > 35% (to avoid noise/leakage from imputation).\n",
    "2. **Add missingness indicators** for features with NA ≥ 5% (to capture “missing is informative”).\n",
    "3. **Impute** remaining NAs with **median per horizon** (robust to outliers, respects horizon differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c6c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-NA features to DROP (>35% NA): ['Attr37']\n",
      "Features to add as MISSING INDICATORS (>=5% NA): ['Attr21', 'Attr27'] ... total 2\n",
      "After indicators+impute+drop → shape: (43405, 67)\n"
     ]
    }
   ],
   "source": [
    "# Compute missingness per feature\n",
    "na_pct = df_all[feature_cols].isna().mean().sort_values(ascending=False)\n",
    "high_na_drop = na_pct[na_pct > 0.35].index.tolist()  # e.g., Attr37\n",
    "indicator_feats = na_pct[(na_pct >= 0.05) & (na_pct <= 0.35)].index.tolist()\n",
    "\n",
    "print('High-NA features to DROP (>35% NA):', high_na_drop)\n",
    "print('Features to add as MISSING INDICATORS (>=5% NA):', indicator_feats[:20], '... total', len(indicator_feats))\n",
    "\n",
    "# Build working dataframe\n",
    "Xy = df_all.copy()\n",
    "for col in indicator_feats:\n",
    "    Xy[f\"{col}__isna\"] = Xy[col].isna().astype(int)\n",
    "\n",
    "# Impute per horizon by median\n",
    "for h in sorted(Xy['horizon'].unique()):\n",
    "    mask = Xy['horizon'] == h\n",
    "    med = Xy.loc[mask, feature_cols].median(numeric_only=True)\n",
    "    Xy.loc[mask, feature_cols] = Xy.loc[mask, feature_cols].fillna(med)\n",
    "\n",
    "# Drop high-NA features\n",
    "Xy = Xy.drop(columns=high_na_drop)\n",
    "\n",
    "print('After indicators+impute+drop → shape:', Xy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170601c7",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- List which columns were dropped (likely `Attr37`).  \n",
    "- Note how many indicator variables were added.  \n",
    "- Confirm the new shape and that `y` still has its original positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae6057",
   "metadata": {},
   "source": [
    "### Step 9 — Outliers: horizon-wise winsorization at 1% / 99%\n",
    "\n",
    "**Why:** Financial ratios are heavy-tailed. Winsorization reduces the impact of extreme values that could destabilize Logit and distort distances.\n",
    "\n",
    "**Plan:** For each horizon, clip each numeric feature at its 1st and 99th percentiles (computed within that horizon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b868fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winsorized numeric columns: 65\n",
      "Example quantiles (pre vs post) for a few columns:\n",
      "  Attr1: pre [-463.89, -0.5767864, 0.049658, 0.6059115999999999, 94.28] → post [-0.6788284, -0.5627992, 0.049658, 0.5602750000000001, 0.6683501999999996]\n",
      "  Attr2: pre [-430.87, 0.01852464, 0.4719, 2.0562319999999983, 480.96] → post [0.01542695, 0.02091588, 0.4719, 1.9171080000000107, 2.478766000000001]\n",
      "  Attr3: pre [-479.96, -1.000576, 0.19661, 0.8897984, 28.336] → post [-1.286353, -0.9036896000000001, 0.19661, 0.8869011999999998, 0.9166460000000005]\n",
      "  Attr4: pre [-0.40311, 0.2001008, 1.5711, 32.30139999999999, 53433.0] → post [0.1466768, 0.2310428, 1.5711, 27.28111999999998, 44.04940000000004]\n",
      "  Attr5: pre [-11903000.0, -2969.528, -0.95364, 2873.8439999999855, 1250100.0] → post [-3458.5980000000004, -2807.7000000000003, -0.95364, 2228.651999999997, 3948.816000000001]\n"
     ]
    }
   ],
   "source": [
    "def winsorize_by_horizon(df: pd.DataFrame, num_cols, lower=0.01, upper=0.99, group_col='horizon'):\n",
    "    df_w = df.copy()\n",
    "    for h, sub in df.groupby(group_col):\n",
    "        q_low = sub[num_cols].quantile(lower)\n",
    "        q_hi  = sub[num_cols].quantile(upper)\n",
    "        idx = df[group_col] == h\n",
    "        df_w.loc[idx, num_cols] = np.clip(df.loc[idx, num_cols], q_low, q_hi, axis=1)\n",
    "    return df_w\n",
    "\n",
    "num_cols = [c for c in Xy.columns if c not in ['y', 'horizon'] and (pd.api.types.is_numeric_dtype(Xy[c]))]\n",
    "Xw = winsorize_by_horizon(Xy, num_cols=num_cols, lower=0.01, upper=0.99, group_col='horizon')\n",
    "\n",
    "print('Winsorized numeric columns:', len(num_cols))\n",
    "print('Example quantiles (pre vs post) for a few columns:')\n",
    "for c in num_cols[:5]:\n",
    "    pre = Xy[c].quantile([0.0,0.01,0.5,0.99,1.0]).to_list()\n",
    "    post = Xw[c].quantile([0.0,0.01,0.5,0.99,1.0]).to_list()\n",
    "    print(f\"  {c}: pre {pre} → post {post}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd0c98",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Check that the extreme min/max values are now truncated while medians remain very similar.  \n",
    "- If any feature behaves strangely (e.g., almost all values clipped), flag it for review/removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efccc13",
   "metadata": {},
   "source": [
    "### Step 10 — Collinearity diagnostics (correlations & VIF)\n",
    "\n",
    "**Why:** Many ratios are algebraically related → unstable coefficients in Logit. We'll:\n",
    "- Identify highly correlated pairs (|r| ≥ 0.9)\n",
    "- Compute **VIF** and flag features with VIF > 10\n",
    "- Propose a reduced set by greedily removing one feature from each high-corr pair (keep features with fewer missingness / lower VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba2f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated pairs (|r| ≥ 0.9): 31\n",
      "[('Attr7', 'Attr14', 0.9999791826597377), ('Attr14', 'Attr18', 0.9995744702753196), ('Attr7', 'Attr18', 0.9993786248329541), ('Attr8', 'Attr17', 0.9971714299955167), ('Attr16', 'Attr26', 0.9939401178067024), ('Attr2', 'Attr10', 0.9879503478873164), ('Attr19', 'Attr23', 0.9865518614198378), ('Attr1', 'Attr7', 0.9824585694091564), ('Attr1', 'Attr14', 0.9824414012994532), ('Attr1', 'Attr18', 0.9818561001579155), ('Attr28', 'Attr54', 0.9772791616999396), ('Attr53', 'Attr54', 0.9761483741374839), ('Attr7', 'Attr11', 0.9738666392999702), ('Attr11', 'Attr14', 0.9738394074905476), ('Attr11', 'Attr18', 0.9732412649272559)]\n",
      "\n",
      "Top VIFs:\n",
      "   feature            VIF\n",
      "13  Attr14  266359.514765\n",
      "6    Attr7  183398.302804\n",
      "17  Attr18    8875.954977\n",
      "7    Attr8     304.159524\n",
      "16  Attr17     299.461469\n",
      "15  Attr16     141.194349\n",
      "25  Attr26     131.652272\n",
      "18  Attr19     128.326955\n",
      "9   Attr10     100.072486\n",
      "1    Attr2      96.743628\n",
      "22  Attr23      79.181790\n",
      "0    Attr1      65.695221\n",
      "52  Attr54      58.212269\n",
      "47  Attr49      36.570410\n",
      "21  Attr22      33.909901\n",
      "40  Attr42      33.736133\n",
      "12  Attr13      32.033796\n",
      "27  Attr28      29.430345\n",
      "44  Attr46      28.224027\n",
      "51  Attr53      27.832978\n",
      "\n",
      "Proposed reduced feature count: 46  (dropped 17 from high-corr pairs)\n"
     ]
    }
   ],
   "source": [
    "# Use the winsorized, imputed features\n",
    "Z = Xw.copy()\n",
    "feat_cols = [c for c in Z.columns if c not in ['y','horizon'] and not c.endswith('__isna')]\n",
    "\n",
    "# Correlation matrix on a sample (for speed if needed)\n",
    "corr = Z[feat_cols].corr().abs()\n",
    "\n",
    "# List highly correlated pairs (upper triangle)\n",
    "high_pairs = []\n",
    "cols = corr.columns\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        r = corr.iloc[i, j]\n",
    "        if r >= 0.9:\n",
    "            high_pairs.append((cols[i], cols[j], float(r)))\n",
    "high_pairs_sorted = sorted(high_pairs, key=lambda x: -x[2])\n",
    "print('Highly correlated pairs (|r| ≥ 0.9):', len(high_pairs_sorted))\n",
    "print(high_pairs_sorted[:15])\n",
    "\n",
    "# VIF (can be slow; run on a subset if needed)\n",
    "import numpy as np\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X_for_vif = Z[feat_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "X_for_vif = add_constant(X_for_vif, has_constant='add')\n",
    "vif_values = []\n",
    "for k, col in enumerate(X_for_vif.columns):\n",
    "    if col == 'const':\n",
    "        continue\n",
    "    vif_values.append((col, variance_inflation_factor(X_for_vif.values, k)))\n",
    "\n",
    "vif_df = pd.DataFrame(vif_values, columns=['feature','VIF']).sort_values('VIF', ascending=False)\n",
    "print('\\nTop VIFs:')\n",
    "print(vif_df.head(20))\n",
    "\n",
    "# Greedy reduction from high-corr pairs\n",
    "to_drop = set()\n",
    "kept = set(feat_cols)\n",
    "for a,b,r in high_pairs_sorted:\n",
    "    if a in to_drop or b in to_drop:\n",
    "        continue\n",
    "    # heuristic: drop the one with higher VIF (if available), else drop second\n",
    "    va = float(vif_df.loc[vif_df['feature']==a, 'VIF'].values[0]) if (vif_df['feature']==a).any() else 1.0\n",
    "    vb = float(vif_df.loc[vif_df['feature']==b, 'VIF'].values[0]) if (vif_df['feature']==b).any() else 1.0\n",
    "    drop = a if va >= vb else b\n",
    "    to_drop.add(drop)\n",
    "    kept.discard(drop)\n",
    "\n",
    "reduced_features = sorted(list(kept))\n",
    "print('\\nProposed reduced feature count:', len(reduced_features), ' (dropped', len(to_drop), 'from high-corr pairs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf56ae1",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Note how many ≥0.9 correlation pairs exist; list any obviously redundant groups.  \n",
    "- Inspect top VIFs — features with VIF > 10 are concerning for Logit inference (not a problem for RF).  \n",
    "- The greedy drop list is a *proposal*; we’ll validate that predictive performance remains stable after reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d9e07",
   "metadata": {},
   "source": [
    "### Step 11 — Save cleaned artifacts\n",
    "\n",
    "**Why:** Keep a consistent cleaned dataset for modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895cf787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: poland_clean_full.parquet and poland_clean_reduced.parquet in /Users/reebal/FH-Wedel/WS25/seminar-bankruptcy-prediction/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Save winsorized, imputed full set\n",
    "full_path = OUT_DIR / 'poland_clean_full.parquet'\n",
    "Z.to_parquet(full_path, index=False)\n",
    "\n",
    "# Also save a reduced-features view\n",
    "reduced_path = OUT_DIR / 'poland_clean_reduced.parquet'\n",
    "cols_out = ['y','horizon'] + reduced_features + [c for c in Z.columns if c.endswith('__isna')]\n",
    "Z[cols_out].to_parquet(reduced_path, index=False)\n",
    "\n",
    "print('Saved:', full_path.name, 'and', reduced_path.name, 'in', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeca06d",
   "metadata": {},
   "source": [
    "**Interpretation (after running):**  \n",
    "- Confirm files were written to `data/processed/`.  \n",
    "- We’ll use `poland_clean_full.parquet` for robust baselines and compare to the `reduced` variant for Logit stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
